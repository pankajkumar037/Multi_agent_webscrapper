{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_in1rx6FX46",
        "outputId": "917fcc99-17ca-427f-c4d5-85aa03d042f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autogen-agentchat\n",
            "  Downloading autogen_agentchat-0.6.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting autogen-core==0.6.4 (from autogen-agentchat)\n",
            "  Downloading autogen_core-0.6.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core==0.6.4->autogen-agentchat)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting opentelemetry-api>=1.34.1 (from autogen-core==0.6.4->autogen-agentchat)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (11.2.1)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (4.14.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.34.1->autogen-core==0.6.4->autogen-agentchat) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.4->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.4->autogen-agentchat) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.4->autogen-agentchat) (0.4.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.6.4->autogen-agentchat) (3.23.0)\n",
            "Downloading autogen_agentchat-0.6.4-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogen_core-0.6.4-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jsonref, opentelemetry-api, autogen-core, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.6.4 autogen-core-0.6.4 jsonref-1.1.0 opentelemetry-api-1.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen-agentchat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install playwright"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y_v3Lb-FX1W",
        "outputId": "765f966f-fa15-4dae-9d60-c894f55ea49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<14,>=13->playwright) (4.14.1)\n",
            "Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl (45.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.53.0 pyee-13.0.0\n"
          ]
        }
      ],
      "source": [
        "!python -m playwright install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSW3WddSFXy3",
        "outputId": "498f5744-ffb4-426e-df4e-1b5fa89e1bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install  beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg39gKERGBmk",
        "outputId": "d1ea73cb-d763-4b71-e0a0-5b56631dd9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromium\n",
            "  Downloading chromium-0.0.0-py3-none-any.whl.metadata (615 bytes)\n",
            "Downloading chromium-0.0.0-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: chromium\n",
            "Successfully installed chromium-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install  chromium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGZT1MNUGbVH",
        "outputId": "5a13a7ff-a117-4d26-ece7-99bf167603c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.9.6-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ag2==0.9.6 (from autogen)\n",
            "  Downloading ag2-0.9.6-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from ag2==0.9.6->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from ag2==0.9.6->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from ag2==0.9.6->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (2.11.7)\n",
            "Collecting python-dotenv (from ag2==0.9.6->autogen)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (0.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (4.14.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.6->autogen) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.6->autogen) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.9.6->autogen) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.6->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.6->autogen) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.6->autogen) (0.4.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->ag2==0.9.6->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->ag2==0.9.6->autogen) (2.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ag2==0.9.6->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->ag2==0.9.6->autogen) (3.4.2)\n",
            "Downloading autogen-0.9.6-py3-none-any.whl (13 kB)\n",
            "Downloading ag2-0.9.6-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m859.2/859.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, diskcache, docker, asyncer, ag2, autogen\n",
            "Successfully installed ag2-0.9.6 asyncer-0.0.8 autogen-0.9.6 diskcache-5.6.3 docker-7.1.0 python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDURhK34GeZu"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "import autogen\n",
        "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
        "\n",
        "# Configuration\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"api_key\": your_api_key,  # Replace with your actual API key\n",
        "        \"base_url\": \"https://api.openai.com/v1\",\n",
        "    }\n",
        "]\n",
        "\n",
        "llm_config = {\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0.1,\n",
        "    \"timeout\": 120,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2WBKpFQcGozV"
      },
      "outputs": [],
      "source": [
        "class SimpleWebScraper:\n",
        "    \"\"\"Simple web scraper using Playwright\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.browser = None\n",
        "        self.context = None\n",
        "        self.results = []\n",
        "\n",
        "    async def setup_browser(self, headless=True):\n",
        "        \"\"\"Initialize browser\"\"\"\n",
        "        playwright = await async_playwright().start()\n",
        "        self.browser = await playwright.chromium.launch(headless=headless)\n",
        "        self.context = await self.browser.new_context(\n",
        "            user_agent=\"Mozilla/5.0 (compatible; DataBot/1.0)\",\n",
        "            viewport={\"width\": 1920, \"height\": 1080}\n",
        "        )\n",
        "\n",
        "    async def scrape_page(self, url: str, selectors: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Scrape a single page\"\"\"\n",
        "        try:\n",
        "            page = await self.context.new_page()\n",
        "            await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
        "\n",
        "            data = {\"url\": url, \"scraped_at\": datetime.now().isoformat()}\n",
        "\n",
        "            for key, selector in selectors.items():\n",
        "                try:\n",
        "                    elements = await page.query_selector_all(selector)\n",
        "                    data[key] = [await el.text_content() for el in elements if await el.text_content()]\n",
        "                except Exception as e:\n",
        "                    data[key] = []\n",
        "                    print(f\"Error scraping {key}: {e}\")\n",
        "\n",
        "            await page.close()\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            return {\"url\": url, \"error\": str(e)}\n",
        "\n",
        "    async def close(self):\n",
        "        \"\"\"Close browser\"\"\"\n",
        "        if self.browser:\n",
        "            await self.browser.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZN2YAHWSGrn7"
      },
      "outputs": [],
      "source": [
        "# Global scraper instance\n",
        "scraper = SimpleWebScraper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "5Ij55HnNFPfA"
      },
      "outputs": [],
      "source": [
        "class MultiAgentWebScrapingSystem:\n",
        "    \"\"\"5-Agent Web Scraping System\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.results = []\n",
        "        self.session_data = {}\n",
        "        self.setup_agents()\n",
        "\n",
        "    def setup_agents(self):\n",
        "        \"\"\"Create the 5 specialized agents\"\"\"\n",
        "\n",
        "        # 1. Master Coordinator Agent\n",
        "        self.coordinator = AssistantAgent(\n",
        "            name=\"MasterCoordinator\",\n",
        "            system_message=\"\"\"You are the Master Coordinator Agent. Your role is to:\n",
        "            1. Analyze user queries and understand what data needs to be scraped\n",
        "            2. Coordinate with other agents to plan and execute scraping tasks\n",
        "            3. Compile and present final results\n",
        "            4. Always start by asking the SearchStrategy agent to analyze the query\n",
        "            5. Distribute tasks to appropriate scraper agents\n",
        "            6. Summarize results from all agents\n",
        "\n",
        "            Format your responses clearly and coordinate the workflow efficiently.\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 2. Search Strategy Agent\n",
        "        self.strategy_agent = AssistantAgent(\n",
        "            name=\"SearchStrategy\",\n",
        "            system_message=\"\"\"You are the Search Strategy Agent. Your role is to:\n",
        "            1. Analyze user queries to understand intent and extract keywords\n",
        "            2. Identify relevant websites and data sources for scraping\n",
        "            3. Generate specific scraping strategies for different types of content\n",
        "            4. Recommend which specialized scraper agents should handle each task\n",
        "            5. Suggest CSS selectors and scraping patterns\n",
        "\n",
        "            Provide detailed scraping plans with:\n",
        "            - Target websites\n",
        "            - Recommended selectors\n",
        "            - Data extraction strategies\n",
        "            - Which scraper agent to use\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 3. Generic Web Scraper Agent\n",
        "        self.generic_scraper = AssistantAgent(\n",
        "            name=\"GenericScraper\",\n",
        "            system_message=\"\"\"You are the Generic Web Scraper Agent. Your role is to:\n",
        "            1. Handle general-purpose web scraping tasks\n",
        "            2. Extract content from various website types\n",
        "            3. Use Playwright to navigate and scrape web pages\n",
        "            4. Report scraping results and any issues encountered\n",
        "\n",
        "            When given a scraping task, provide:\n",
        "            - Confirmation of the task received\n",
        "            - Scraping approach and selectors to use\n",
        "            - Results summary after scraping\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 4. Data Processing Agent\n",
        "        self.data_processor = AssistantAgent(\n",
        "            name=\"DataProcessor\",\n",
        "            system_message=\"\"\"You are the Data Processing Agent. Your role is to:\n",
        "            1. Clean and structure scraped data\n",
        "            2. Remove duplicates and invalid entries\n",
        "            3. Standardize data formats\n",
        "            4. Organize data into structured formats\n",
        "            5. Perform basic data validation\n",
        "\n",
        "            When processing data, provide:\n",
        "            - Data cleaning summary\n",
        "            - Structure improvements made\n",
        "            - Final processed data format\n",
        "            - Data quality assessment\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 5. Quality Assurance Agent\n",
        "        self.qa_agent = AssistantAgent(\n",
        "            name=\"QualityAssurance\",\n",
        "            system_message=\"\"\"You are the Quality Assurance Agent. Your role is to:\n",
        "            1. Verify data accuracy and completeness\n",
        "            2. Check for inconsistencies across scraped data\n",
        "            3. Validate data quality metrics\n",
        "            4. Flag potential errors or issues\n",
        "            5. Provide quality scores and recommendations\n",
        "\n",
        "            When reviewing data, provide:\n",
        "            - Quality assessment report\n",
        "            - Identified issues and inconsistencies\n",
        "            - Recommendations for improvement\n",
        "            - Overall quality score (1-10)\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # User proxy for human interaction\n",
        "        self.user_proxy = UserProxyAgent(\n",
        "            name=\"UserProxy\",\n",
        "            system_message=\"You facilitate communication between the user and the agent system.\",\n",
        "            code_execution_config={\"work_dir\": \"scraping_work\", \"use_docker\": False},\n",
        "            human_input_mode=\"NEVER\"\n",
        "        )\n",
        "\n",
        "        # Create group chat\n",
        "        self.agents = [\n",
        "            self.coordinator,\n",
        "            self.strategy_agent,\n",
        "            self.generic_scraper,\n",
        "            self.data_processor,\n",
        "            self.qa_agent\n",
        "        ]\n",
        "\n",
        "        self.groupchat = GroupChat(\n",
        "            agents=self.agents,\n",
        "            messages=[],\n",
        "            max_round=15,\n",
        "            speaker_selection_method=\"round_robin\"\n",
        "        )\n",
        "\n",
        "        self.manager = GroupChatManager(\n",
        "            groupchat=self.groupchat,\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "    async def execute_scraping_task(self, query: str):\n",
        "        \"\"\"Execute the multi-agent scraping workflow\"\"\"\n",
        "\n",
        "        print(f\" Starting multi-agent scraping for query: '{query}'\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Start the conversation\n",
        "        initial_message = f\"\"\"\n",
        "        New scraping request: \"{query}\"\n",
        "\n",
        "        Please coordinate to:\n",
        "        1. Analyze the query and determine scraping strategy\n",
        "        2. Identify target websites and data sources\n",
        "        3. Execute scraping tasks\n",
        "        4. Process and clean the data\n",
        "        5. Perform quality assurance\n",
        "        6. Provide final results\n",
        "\n",
        "        Let's begin with the SearchStrategy agent analyzing this query.\n",
        "        \"\"\"\n",
        "\n",
        "        # Execute the group chat\n",
        "        await self.user_proxy.a_initiate_chat(\n",
        "            self.manager,\n",
        "            message=initial_message\n",
        "        )\n",
        "\n",
        "        return self.groupchat.messages\n",
        "\n",
        "    async def scrape_with_agents(self, query: str, urls: List[str] = None, selectors: Dict[str, str] = None):\n",
        "        \"\"\"Simplified scraping function for demo purposes\"\"\"\n",
        "\n",
        "        if not urls:\n",
        "            # Default example URLs for demonstration\n",
        "            urls = [\n",
        "                \"https://example.com\",\n",
        "                \"https://httpbin.org/html\"\n",
        "            ]\n",
        "\n",
        "        if not selectors:\n",
        "            # Default selectors\n",
        "            selectors = {\n",
        "                \"title\": \"title\",\n",
        "                \"headings\": \"h1, h2, h3\",\n",
        "                \"paragraphs\": \"p\",\n",
        "                \"links\": \"a\",\n",
        "                \"code_blocks\": \"pre, code\"\n",
        "            }\n",
        "\n",
        "        # Setup browser\n",
        "        await scraper.setup_browser()\n",
        "\n",
        "        results = []\n",
        "        for url in urls:\n",
        "            print(f\"üìÑ Scraping: {url}\")\n",
        "            data = await scraper.scrape_page(url, selectors)\n",
        "            results.append(data)\n",
        "\n",
        "        await scraper.close()\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wM8DYzz0HDQ1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_custom_scraping_session(query: str, urls: List[str], selectors: Dict[str, str]):\n",
        "    \"\"\"Create a custom scraping session\"\"\"\n",
        "\n",
        "    async def run_custom_session():\n",
        "        system = MultiAgentWebScrapingSystem()\n",
        "\n",
        "        # First, let agents analyze the query\n",
        "        messages = await system.execute_scraping_task(query)\n",
        "\n",
        "        # Then perform actual scraping\n",
        "        scraping_results = await system.scrape_with_agents(query, urls, selectors)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"agent_discussion\": messages,\n",
        "            \"scraping_results\": scraping_results,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    return run_custom_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EcHSrfGALlm1",
        "outputId": "aeddf359-64c7-49f2-ede6-6df00a87ffe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting multi-agent scraping for query: 'scrap content on page'\n",
            "============================================================\n",
            "UserProxy (to chat_manager):\n",
            "\n",
            "\n",
            "        New scraping request: \"scrap content on page\"\n",
            "        \n",
            "        Please coordinate to:\n",
            "        1. Analyze the query and determine scraping strategy\n",
            "        2. Identify target websites and data sources\n",
            "        3. Execute scraping tasks\n",
            "        4. Process and clean the data\n",
            "        5. Perform quality assurance\n",
            "        6. Provide final results\n",
            "        \n",
            "        Let's begin with the SearchStrategy agent analyzing this query.\n",
            "        \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: MasterCoordinator\n",
            "\n",
            "MasterCoordinator (to chat_manager):\n",
            "\n",
            "SearchStrategy Agent, could you please analyze the user's query and propose a suitable scraping strategy? The user wants to scrape content from a page but hasn't specified which page or what kind of content. We may need to ask for more details.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: SearchStrategy\n",
            "\n",
            "SearchStrategy (to chat_manager):\n",
            "\n",
            "Indeed, the user's query is quite broad and lacks specific details. To formulate an effective scraping strategy, we need to know:\n",
            "\n",
            "1. The specific website(s) or page(s) from which the user wants to scrape content.\n",
            "2. The type of content the user is interested in (e.g., text, images, videos, metadata, etc.).\n",
            "3. The purpose of the scraping, which can help us understand the importance of different types of data.\n",
            "\n",
            "I recommend that we ask the user to provide more details about their scraping needs. Once we have this information, we can identify the target websites, recommend the appropriate selectors, and suggest the best scraper agents for the task.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: GenericScraper\n",
            "\n",
            "GenericScraper (to chat_manager):\n",
            "\n",
            "Thank you, SearchStrategy Agent. I agree with your assessment. \n",
            "\n",
            "User, could you please provide more details about your scraping needs? Specifically, we need to know:\n",
            "\n",
            "1. The specific website(s) or page(s) from which you want to scrape content.\n",
            "2. The type of content you are interested in (e.g., text, images, videos, metadata, etc.).\n",
            "3. The purpose of the scraping, which can help us understand the importance of different types of data.\n",
            "\n",
            "Once we have this information, we can proceed with your request more effectively.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: DataProcessor\n",
            "\n",
            "DataProcessor (to chat_manager):\n",
            "\n",
            "As a Data Processing Agent, I'll wait for the user to provide more specific details about the scraping task. Once the data is scraped and provided by the GenericScraper, I will:\n",
            "\n",
            "1. Clean the data: This involves removing any irrelevant information, correcting any inaccuracies, and dealing with any missing data.\n",
            "\n",
            "2. Structure the data: I will organize the data into a structured format that is easy to understand and analyze. This could involve creating tables, charts, or other visual representations of the data.\n",
            "\n",
            "3. Remove duplicates: If there are any duplicate entries in the data, I will remove them to ensure that each piece of data is unique.\n",
            "\n",
            "4. Standardize data formats: If the data comes in different formats, I will standardize it into a single, consistent format.\n",
            "\n",
            "5. Perform basic data validation: I will check the data to ensure it is accurate, complete, and meets the user's requirements.\n",
            "\n",
            "Once these steps are completed, I will provide a summary of the data cleaning process, detail any improvements made to the structure of the data, describe the final processed data format, and assess the quality of the data.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: QualityAssurance\n",
            "\n",
            "QualityAssurance (to chat_manager):\n",
            "\n",
            "As a Quality Assurance Agent, I will review the data after it has been processed by the DataProcessor. My tasks will include:\n",
            "\n",
            "1. Verifying data accuracy: I will cross-check the data with the original source or other reliable sources to ensure it is accurate.\n",
            "\n",
            "2. Checking for inconsistencies: I will look for any inconsistencies in the data, such as discrepancies between similar data points or irregularities in data patterns.\n",
            "\n",
            "3. Validating data quality metrics: I will assess the data based on various quality metrics, such as completeness, uniqueness, timeliness, validity, and consistency.\n",
            "\n",
            "4. Flagging potential errors or issues: If I identify any potential errors or issues in the data, I will flag them for review.\n",
            "\n",
            "5. Providing quality scores and recommendations: Based on my assessment, I will provide a quality score for the data (on a scale of 1 to 10) and make recommendations for improving data quality.\n",
            "\n",
            "Once I have completed these tasks, I will provide a quality assessment report detailing my findings and recommendations.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: MasterCoordinator\n",
            "\n",
            "MasterCoordinator (to chat_manager):\n",
            "\n",
            "Thank you, everyone, for your inputs. \n",
            "\n",
            "User, as you can see, we need more specific details to proceed with your request. Could you please provide:\n",
            "\n",
            "1. The specific website(s) or page(s) from which you want to scrape content.\n",
            "2. The type of content you are interested in (e.g., text, images, videos, metadata, etc.).\n",
            "3. The purpose of the scraping, which can help us understand the importance of different types of data.\n",
            "\n",
            "Once we have this information, we can proceed with your request more effectively.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: SearchStrategy\n",
            "\n",
            "SearchStrategy (to chat_manager):\n",
            "\n",
            "As the Search Strategy Agent, I agree with the MasterCoordinator's request for more information. Once we have more specific details, I can provide a detailed scraping plan including:\n",
            "\n",
            "- Target websites\n",
            "- Recommended selectors\n",
            "- Data extraction strategies\n",
            "- Which scraper agent to use\n",
            "\n",
            "This will ensure we can effectively and efficiently fulfill the user's request.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: GenericScraper\n",
            "\n",
            "GenericScraper (to chat_manager):\n",
            "\n",
            "Understood. I'm ready to proceed once we receive more specific details about the scraping task. With the right information, I can navigate to the target website(s), use the appropriate selectors to locate the desired content, and extract it for further processing. I'll also report any issues encountered during the scraping process.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: DataProcessor\n",
            "\n",
            "DataProcessor (to chat_manager):\n",
            "\n",
            "As the Data Processing Agent, I'm ready to process and clean the data once it's scraped by the GenericScraper. With more specific details, I can better prepare for the types of data cleaning and structuring tasks that will be required. This could include preparing to handle specific data formats, planning for potential data quality issues, and setting up the necessary tools and processes for data cleaning and structuring. I'll also be ready to remove duplicates, standardize data formats, and perform basic data validation.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: QualityAssurance\n",
            "\n",
            "QualityAssurance (to chat_manager):\n",
            "\n",
            "As the Quality Assurance Agent, I'm prepared to review the data after it has been processed by the DataProcessor. With more specific details, I can better anticipate potential data quality issues and prepare appropriate quality checks. I'll be ready to verify data accuracy, check for inconsistencies, validate data quality metrics, flag potential errors or issues, and provide quality scores and recommendations. Once I've completed these tasks, I'll provide a quality assessment report detailing my findings and recommendations.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: MasterCoordinator\n",
            "\n",
            "MasterCoordinator (to chat_manager):\n",
            "\n",
            "Thank you, everyone, for your readiness and detailed plans. \n",
            "\n",
            "User, we are waiting for your response with more specific details about your scraping needs. Once we have this information, we can proceed with your request more effectively.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: SearchStrategy\n",
            "\n",
            "SearchStrategy (to chat_manager):\n",
            "\n",
            "As the Search Strategy Agent, I agree with the MasterCoordinator's request for more information. Once we have more specific details, I can provide a detailed scraping plan including:\n",
            "\n",
            "- Target websites\n",
            "- Recommended selectors\n",
            "- Data extraction strategies\n",
            "- Which scraper agent to use\n",
            "\n",
            "This will ensure we can effectively and efficiently fulfill the user's request.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: GenericScraper\n",
            "\n",
            "GenericScraper (to chat_manager):\n",
            "\n",
            "Understood. I'm ready to proceed once we receive more specific details about the scraping task. With the right information, I can navigate to the target website(s), use the appropriate selectors to locate the desired content, and extract it for further processing. I'll also report any issues encountered during the scraping process.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: DataProcessor\n",
            "\n",
            "DataProcessor (to chat_manager):\n",
            "\n",
            "As the Data Processing Agent, I'm ready to process and clean the data once it's scraped by the GenericScraper. With more specific details, I can better prepare for the types of data cleaning and structuring tasks that will be required. This could include preparing to handle specific data formats, planning for potential data quality issues, and setting up the necessary tools and processes for data cleaning and structuring. I'll also be ready to remove duplicates, standardize data formats, and perform basic data validation.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (1a8fb034-fa10-4b3c-943c-ae9405487a76): Maximum rounds (15) reached\n",
            "üìÑ Scraping: https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110\n"
          ]
        }
      ],
      "source": [
        "# # Custom scraping session\n",
        "urls = [\"https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110\"]\n",
        "selectors = {\"title\": \"title\", \"headings\": \"h1, h2, h3\", \"paragraphs\": \"p\",\"code_blocks\": \"pre, code\"}\n",
        "\n",
        "result = await create_custom_scraping_session(\"scrap content on page\",selectors=selectors,urls=urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XjeXHvRzLljV",
        "outputId": "2d850563-bd5e-4e0f-9d3a-6b88ce98f162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Chromium 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G171.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 0% 21.9s\u001b[0K\u001b[1G171.6 MiB [] 0% 13.5s\u001b[0K\u001b[1G171.6 MiB [] 0% 7.2s\u001b[0K\u001b[1G171.6 MiB [] 1% 4.5s\u001b[0K\u001b[1G171.6 MiB [] 2% 3.9s\u001b[0K\u001b[1G171.6 MiB [] 2% 3.2s\u001b[0K\u001b[1G171.6 MiB [] 3% 2.8s\u001b[0K\u001b[1G171.6 MiB [] 4% 2.7s\u001b[0K\u001b[1G171.6 MiB [] 5% 2.9s\u001b[0K\u001b[1G171.6 MiB [] 5% 2.8s\u001b[0K\u001b[1G171.6 MiB [] 6% 2.8s\u001b[0K\u001b[1G171.6 MiB [] 7% 2.7s\u001b[0K\u001b[1G171.6 MiB [] 8% 2.6s\u001b[0K\u001b[1G171.6 MiB [] 9% 2.5s\u001b[0K\u001b[1G171.6 MiB [] 10% 2.6s\u001b[0K\u001b[1G171.6 MiB [] 11% 2.5s\u001b[0K\u001b[1G171.6 MiB [] 12% 2.4s\u001b[0K\u001b[1G171.6 MiB [] 13% 2.3s\u001b[0K\u001b[1G171.6 MiB [] 14% 2.1s\u001b[0K\u001b[1G171.6 MiB [] 15% 2.1s\u001b[0K\u001b[1G171.6 MiB [] 16% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 17% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 18% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 19% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 20% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 20% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 21% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 22% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 23% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 24% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 25% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 26% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 26% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 27% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 28% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 29% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 30% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 31% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 32% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 33% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 33% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 34% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 35% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 36% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 37% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 38% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 39% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 39% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 40% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 41% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 43% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 44% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 46% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 47% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 49% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 50% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 51% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 52% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 54% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 55% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 56% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 57% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 59% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 60% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 61% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 62% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 63% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 64% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 65% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 66% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 67% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 68% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 69% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 70% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 71% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 72% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 73% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 74% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 75% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 76% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 77% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 78% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 79% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 80% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 82% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 83% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 84% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 85% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 86% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 88% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 90% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 91% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 92% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 92% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 95% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 97% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 98% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium-1179\n",
            "Downloading Chromium Headless Shell 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 0% 18.6s\u001b[0K\u001b[1G104.5 MiB [] 0% 18.2s\u001b[0K\u001b[1G104.5 MiB [] 0% 18.4s\u001b[0K\u001b[1G104.5 MiB [] 0% 10.2s\u001b[0K\u001b[1G104.5 MiB [] 0% 8.9s\u001b[0K\u001b[1G104.5 MiB [] 1% 5.9s\u001b[0K\u001b[1G104.5 MiB [] 2% 5.4s\u001b[0K\u001b[1G104.5 MiB [] 2% 5.3s\u001b[0K\u001b[1G104.5 MiB [] 3% 5.3s\u001b[0K\u001b[1G104.5 MiB [] 3% 5.1s\u001b[0K\u001b[1G104.5 MiB [] 4% 4.6s\u001b[0K\u001b[1G104.5 MiB [] 4% 4.2s\u001b[0K\u001b[1G104.5 MiB [] 5% 4.0s\u001b[0K\u001b[1G104.5 MiB [] 6% 3.6s\u001b[0K\u001b[1G104.5 MiB [] 7% 3.4s\u001b[0K\u001b[1G104.5 MiB [] 7% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 8% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 8% 3.4s\u001b[0K\u001b[1G104.5 MiB [] 8% 3.5s\u001b[0K\u001b[1G104.5 MiB [] 9% 3.5s\u001b[0K\u001b[1G104.5 MiB [] 9% 3.4s\u001b[0K\u001b[1G104.5 MiB [] 10% 3.4s\u001b[0K\u001b[1G104.5 MiB [] 11% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 12% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 13% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 13% 3.2s\u001b[0K\u001b[1G104.5 MiB [] 13% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 14% 3.2s\u001b[0K\u001b[1G104.5 MiB [] 15% 3.2s\u001b[0K\u001b[1G104.5 MiB [] 15% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 16% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 17% 3.2s\u001b[0K\u001b[1G104.5 MiB [] 18% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 19% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 20% 3.0s\u001b[0K\u001b[1G104.5 MiB [] 20% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 21% 3.0s\u001b[0K\u001b[1G104.5 MiB [] 22% 3.0s\u001b[0K\u001b[1G104.5 MiB [] 22% 2.9s\u001b[0K\u001b[1G104.5 MiB [] 23% 3.0s\u001b[0K\u001b[1G104.5 MiB [] 24% 2.9s\u001b[0K\u001b[1G104.5 MiB [] 26% 2.8s\u001b[0K\u001b[1G104.5 MiB [] 27% 2.7s\u001b[0K\u001b[1G104.5 MiB [] 28% 2.6s\u001b[0K\u001b[1G104.5 MiB [] 29% 2.6s\u001b[0K\u001b[1G104.5 MiB [] 30% 2.5s\u001b[0K\u001b[1G104.5 MiB [] 31% 2.5s\u001b[0K\u001b[1G104.5 MiB [] 32% 2.5s\u001b[0K\u001b[1G104.5 MiB [] 32% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 33% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 34% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 34% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 35% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 36% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 37% 2.2s\u001b[0K\u001b[1G104.5 MiB [] 38% 2.2s\u001b[0K\u001b[1G104.5 MiB [] 40% 2.0s\u001b[0K\u001b[1G104.5 MiB [] 41% 2.0s\u001b[0K\u001b[1G104.5 MiB [] 43% 1.9s\u001b[0K\u001b[1G104.5 MiB [] 44% 1.8s\u001b[0K\u001b[1G104.5 MiB [] 45% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 46% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 47% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 49% 1.6s\u001b[0K\u001b[1G104.5 MiB [] 49% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 50% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 51% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 52% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 53% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 54% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 55% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 56% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 57% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 58% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 59% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 61% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 62% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 63% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 64% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 65% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 66% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 68% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 69% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 70% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 72% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 73% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 74% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 75% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 76% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 77% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 78% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 79% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 80% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 81% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 82% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 83% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 85% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 86% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 87% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 88% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 89% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 90% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 91% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 92% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 93% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 94% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1179\n",
            "Downloading Firefox 139.0 (playwright build v1488)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1488/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G92.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 0% 22.2s\u001b[0K\u001b[1G92.3 MiB [] 0% 9.4s\u001b[0K\u001b[1G92.3 MiB [] 0% 6.9s\u001b[0K\u001b[1G92.3 MiB [] 1% 4.4s\u001b[0K\u001b[1G92.3 MiB [] 2% 3.6s\u001b[0K\u001b[1G92.3 MiB [] 3% 3.3s\u001b[0K\u001b[1G92.3 MiB [] 4% 2.9s\u001b[0K\u001b[1G92.3 MiB [] 5% 2.5s\u001b[0K\u001b[1G92.3 MiB [] 6% 2.3s\u001b[0K\u001b[1G92.3 MiB [] 7% 2.2s\u001b[0K\u001b[1G92.3 MiB [] 8% 2.0s\u001b[0K\u001b[1G92.3 MiB [] 9% 2.0s\u001b[0K\u001b[1G92.3 MiB [] 10% 2.2s\u001b[0K\u001b[1G92.3 MiB [] 11% 2.2s\u001b[0K\u001b[1G92.3 MiB [] 11% 2.3s\u001b[0K\u001b[1G92.3 MiB [] 12% 2.3s\u001b[0K\u001b[1G92.3 MiB [] 13% 2.1s\u001b[0K\u001b[1G92.3 MiB [] 13% 2.2s\u001b[0K\u001b[1G92.3 MiB [] 14% 2.1s\u001b[0K\u001b[1G92.3 MiB [] 15% 2.0s\u001b[0K\u001b[1G92.3 MiB [] 16% 2.1s\u001b[0K\u001b[1G92.3 MiB [] 17% 2.0s\u001b[0K\u001b[1G92.3 MiB [] 19% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 20% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 21% 1.8s\u001b[0K\u001b[1G92.3 MiB [] 22% 1.8s\u001b[0K\u001b[1G92.3 MiB [] 24% 1.7s\u001b[0K\u001b[1G92.3 MiB [] 25% 1.6s\u001b[0K\u001b[1G92.3 MiB [] 26% 1.6s\u001b[0K\u001b[1G92.3 MiB [] 28% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 29% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 30% 1.4s\u001b[0K\u001b[1G92.3 MiB [] 31% 1.4s\u001b[0K\u001b[1G92.3 MiB [] 32% 1.3s\u001b[0K\u001b[1G92.3 MiB [] 33% 1.3s\u001b[0K\u001b[1G92.3 MiB [] 35% 1.3s\u001b[0K\u001b[1G92.3 MiB [] 36% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 37% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 38% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 39% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 41% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 42% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 43% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 45% 1.0s\u001b[0K\u001b[1G92.3 MiB [] 46% 1.0s\u001b[0K\u001b[1G92.3 MiB [] 48% 1.0s\u001b[0K\u001b[1G92.3 MiB [] 49% 0.9s\u001b[0K\u001b[1G92.3 MiB [] 51% 0.9s\u001b[0K\u001b[1G92.3 MiB [] 52% 0.9s\u001b[0K\u001b[1G92.3 MiB [] 54% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 55% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 57% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 58% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 59% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 61% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 62% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 63% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 65% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 67% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 68% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 69% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 71% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 72% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 74% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 76% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 78% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 80% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 81% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 82% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 83% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 85% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 86% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 88% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 90% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 96% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 139.0 (playwright build v1488) downloaded to /root/.cache/ms-playwright/firefox-1488\n",
            "Downloading Webkit 18.5 (playwright build v2182)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2182/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G93.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 0% 7.1s\u001b[0K\u001b[1G93.7 MiB [] 0% 3.7s\u001b[0K\u001b[1G93.7 MiB [] 2% 2.4s\u001b[0K\u001b[1G93.7 MiB [] 3% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 4% 1.5s\u001b[0K\u001b[1G93.7 MiB [] 6% 1.4s\u001b[0K\u001b[1G93.7 MiB [] 8% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 9% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 10% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 11% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 12% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 14% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 15% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 16% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 18% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 20% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 22% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 24% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 26% 0.9s\u001b[0K\u001b[1G93.7 MiB [] 28% 0.9s\u001b[0K\u001b[1G93.7 MiB [] 30% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 32% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 34% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 36% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 38% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 39% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 41% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 42% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 45% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 47% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 49% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 52% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 54% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 56% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 57% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 58% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 60% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 62% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 64% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 66% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 69% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 71% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 73% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 76% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 78% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 80% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 85% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 87% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 92% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 95% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.5 (playwright build v2182) downloaded to /root/.cache/ms-playwright/webkit-2182\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 21% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 67% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë Host system is missing dependencies to run browsers. ‚ïë\n",
            "‚ïë Missing libraries:                                   ‚ïë\n",
            "‚ïë     libwoff2dec.so.1.0.2                             ‚ïë\n",
            "‚ïë     libgstgl-1.0.so.0                                ‚ïë\n",
            "‚ïë     libgstcodecparsers-1.0.so.0                      ‚ïë\n",
            "‚ïë     libavif.so.13                                    ‚ïë\n",
            "‚ïë     libharfbuzz-icu.so.0                             ‚ïë\n",
            "‚ïë     libenchant-2.so.2                                ‚ïë\n",
            "‚ïë     libsecret-1.so.0                                 ‚ïë\n",
            "‚ïë     libhyphen.so.0                                   ‚ïë\n",
            "‚ïë     libmanette-0.2.so.0                              ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:927:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1049:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1038:7)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dQJm9VOLLlg6",
        "outputId": "6fbef44a-84db-4ffb-c0ac-542d73fd0a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'query': 'scrap content on page', 'agent_discussion': [{'content': '\\n        New scraping request: \"scrap content on page\"\\n        \\n        Please coordinate to:\\n        1. Analyze the query and determine scraping strategy\\n        2. Identify target websites and data sources\\n        3. Execute scraping tasks\\n        4. Process and clean the data\\n        5. Perform quality assurance\\n        6. Provide final results\\n        \\n        Let\\'s begin with the SearchStrategy agent analyzing this query.\\n        ', 'role': 'user', 'name': 'UserProxy'}, {'content': \"SearchStrategy Agent, could you please analyze the user's query and propose a suitable scraping strategy? The user wants to scrape content from a page but hasn't specified which page or what kind of content. We may need to ask for more details.\", 'role': 'user', 'name': 'MasterCoordinator'}, {'content': \"Indeed, the user's query is quite broad and lacks specific details. To formulate an effective scraping strategy, we need to know:\\n\\n1. The specific website(s) or page(s) from which the user wants to scrape content.\\n2. The type of content the user is interested in (e.g., text, images, videos, metadata, etc.).\\n3. The purpose of the scraping, which can help us understand the importance of different types of data.\\n\\nI recommend that we ask the user to provide more details about their scraping needs. Once we have this information, we can identify the target websites, recommend the appropriate selectors, and suggest the best scraper agents for the task.\", 'role': 'user', 'name': 'SearchStrategy'}, {'content': 'Thank you, SearchStrategy Agent. I agree with your assessment. \\n\\nUser, could you please provide more details about your scraping needs? Specifically, we need to know:\\n\\n1. The specific website(s) or page(s) from which you want to scrape content.\\n2. The type of content you are interested in (e.g., text, images, videos, metadata, etc.).\\n3. The purpose of the scraping, which can help us understand the importance of different types of data.\\n\\nOnce we have this information, we can proceed with your request more effectively.', 'role': 'user', 'name': 'GenericScraper'}, {'content': \"As a Data Processing Agent, I'll wait for the user to provide more specific details about the scraping task. Once the data is scraped and provided by the GenericScraper, I will:\\n\\n1. Clean the data: This involves removing any irrelevant information, correcting any inaccuracies, and dealing with any missing data.\\n\\n2. Structure the data: I will organize the data into a structured format that is easy to understand and analyze. This could involve creating tables, charts, or other visual representations of the data.\\n\\n3. Remove duplicates: If there are any duplicate entries in the data, I will remove them to ensure that each piece of data is unique.\\n\\n4. Standardize data formats: If the data comes in different formats, I will standardize it into a single, consistent format.\\n\\n5. Perform basic data validation: I will check the data to ensure it is accurate, complete, and meets the user's requirements.\\n\\nOnce these steps are completed, I will provide a summary of the data cleaning process, detail any improvements made to the structure of the data, describe the final processed data format, and assess the quality of the data.\", 'role': 'user', 'name': 'DataProcessor'}, {'content': 'As a Quality Assurance Agent, I will review the data after it has been processed by the DataProcessor. My tasks will include:\\n\\n1. Verifying data accuracy: I will cross-check the data with the original source or other reliable sources to ensure it is accurate.\\n\\n2. Checking for inconsistencies: I will look for any inconsistencies in the data, such as discrepancies between similar data points or irregularities in data patterns.\\n\\n3. Validating data quality metrics: I will assess the data based on various quality metrics, such as completeness, uniqueness, timeliness, validity, and consistency.\\n\\n4. Flagging potential errors or issues: If I identify any potential errors or issues in the data, I will flag them for review.\\n\\n5. Providing quality scores and recommendations: Based on my assessment, I will provide a quality score for the data (on a scale of 1 to 10) and make recommendations for improving data quality.\\n\\nOnce I have completed these tasks, I will provide a quality assessment report detailing my findings and recommendations.', 'role': 'user', 'name': 'QualityAssurance'}, {'content': 'Thank you, everyone, for your inputs. \\n\\nUser, as you can see, we need more specific details to proceed with your request. Could you please provide:\\n\\n1. The specific website(s) or page(s) from which you want to scrape content.\\n2. The type of content you are interested in (e.g., text, images, videos, metadata, etc.).\\n3. The purpose of the scraping, which can help us understand the importance of different types of data.\\n\\nOnce we have this information, we can proceed with your request more effectively.', 'role': 'user', 'name': 'MasterCoordinator'}, {'content': \"As the Search Strategy Agent, I agree with the MasterCoordinator's request for more information. Once we have more specific details, I can provide a detailed scraping plan including:\\n\\n- Target websites\\n- Recommended selectors\\n- Data extraction strategies\\n- Which scraper agent to use\\n\\nThis will ensure we can effectively and efficiently fulfill the user's request.\", 'role': 'user', 'name': 'SearchStrategy'}, {'content': \"Understood. I'm ready to proceed once we receive more specific details about the scraping task. With the right information, I can navigate to the target website(s), use the appropriate selectors to locate the desired content, and extract it for further processing. I'll also report any issues encountered during the scraping process.\", 'role': 'user', 'name': 'GenericScraper'}, {'content': \"As the Data Processing Agent, I'm ready to process and clean the data once it's scraped by the GenericScraper. With more specific details, I can better prepare for the types of data cleaning and structuring tasks that will be required. This could include preparing to handle specific data formats, planning for potential data quality issues, and setting up the necessary tools and processes for data cleaning and structuring. I'll also be ready to remove duplicates, standardize data formats, and perform basic data validation.\", 'role': 'user', 'name': 'DataProcessor'}, {'content': \"As the Quality Assurance Agent, I'm prepared to review the data after it has been processed by the DataProcessor. With more specific details, I can better anticipate potential data quality issues and prepare appropriate quality checks. I'll be ready to verify data accuracy, check for inconsistencies, validate data quality metrics, flag potential errors or issues, and provide quality scores and recommendations. Once I've completed these tasks, I'll provide a quality assessment report detailing my findings and recommendations.\", 'role': 'user', 'name': 'QualityAssurance'}, {'content': 'Thank you, everyone, for your readiness and detailed plans. \\n\\nUser, we are waiting for your response with more specific details about your scraping needs. Once we have this information, we can proceed with your request more effectively.', 'role': 'user', 'name': 'MasterCoordinator'}, {'content': \"As the Search Strategy Agent, I agree with the MasterCoordinator's request for more information. Once we have more specific details, I can provide a detailed scraping plan including:\\n\\n- Target websites\\n- Recommended selectors\\n- Data extraction strategies\\n- Which scraper agent to use\\n\\nThis will ensure we can effectively and efficiently fulfill the user's request.\", 'role': 'user', 'name': 'SearchStrategy'}, {'content': \"Understood. I'm ready to proceed once we receive more specific details about the scraping task. With the right information, I can navigate to the target website(s), use the appropriate selectors to locate the desired content, and extract it for further processing. I'll also report any issues encountered during the scraping process.\", 'role': 'user', 'name': 'GenericScraper'}, {'content': \"As the Data Processing Agent, I'm ready to process and clean the data once it's scraped by the GenericScraper. With more specific details, I can better prepare for the types of data cleaning and structuring tasks that will be required. This could include preparing to handle specific data formats, planning for potential data quality issues, and setting up the necessary tools and processes for data cleaning and structuring. I'll also be ready to remove duplicates, standardize data formats, and perform basic data validation.\", 'role': 'user', 'name': 'DataProcessor'}], 'scraping_results': [{'url': 'https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110', 'scraped_at': '2025-07-15T13:59:41.931227', 'title': ['Intro to Web Scraping: Build Your First Scraper in 5 Minutes | by Joe Osborne | Medium'], 'headings': ['Intro to Web Scraping: Build Your First Scraper in 5 Minutes', 'A quick guide to help you build a simple web scraper.', 'Step 1', 'Step 2', 'Step 3', 'Resources', 'Sign up to discover human stories that deepen your understanding of the world.', 'Free', 'Membership', 'Written by Joe Osborne', 'Responses (9)', 'More from Joe Osborne', 'How to Log Every Request and Response in FastAPI', 'üìï A guide on efficiently storing logs for every request and response in your FastAPI app', 'FastAPI with API Key Authentication', 'üìò How to set up your FastAPI app with a simple API key auth system.', 'Easy Web Scraping: Scrape an Entire Website in Under 10 Minutes', 'üìï A simple guide on building and running a web scraper.', 'Guide: How to Build a Customized AI Assistant using OpenAI‚Äôs Assistants API', 'An in-depth guide on how to create an assistant, create vector stores with your own documents, and add custom functions that the assistant‚Ä¶', 'Recommended from Medium', '5 Python-Powered AI Prompts That Quietly Replace Full-Time Jobs', 'These Python-powered automations quietly outperform entire workflows\\u200a‚Äî\\u200aand you can build them in a weekend.', 'Easy Web Scraping: Scrape an Entire Website in Under 10 Minutes', 'üìï A simple guide on building and running a web scraper.', '5 Python Automation Projects You Can Build This Weekend (And Actually Use)', 'These projects will boost your resume', 'üíÉ How a Vietnamese Developer Grew an AI App to $20,000/Month in a Few Months Using TikTok', 'Hello! I‚Äôm Yuma Ueno(https://twitter.com/stat_biz).', 'Find and Validate Startup Ideas in Minutes Using This AI Tool (Free)', 'I wish I had found out about this earlier', 'I Built a Side Hustle with AI. Now It Pays Me $800 every month', '$800/Month from My Laptop (No BS )'], 'paragraphs': ['Sign up', 'Sign in', 'Sign up', 'Sign in', '1', 'Top highlight', '146', '9', 'Listen', 'Share', 'Web scraping is my favorite area of coding. It‚Äôs both incredibly frustrating and extremely rewarding. The best thing about scraping is that the possibilities are endless. There‚Äôs virtually infinite data on the internet, and a lot of it is super easy to collect with a simple scraper.', 'Getting started is very easy. When I began, I had only written a few lines of code in my entire life, but I had the idea to scrape mortgage interest rates to watch their trend in real time. After a quick Google search and about 20 minutes later, I had built my first web scraper. I want to help others do the same!', 'I typically use either Python or JavaScript/TypeScript. Both are great options, it‚Äôs mostly a matter of preference. In this guide I‚Äôll go over very simple examples for both.', 'Prerequisites:', 'That‚Äôs it! Let‚Äôs get started.', 'Create a new folder on your machine and a new .js or .py file inside the folder. Let‚Äôs name them scraper-python.pyand scraper-javascript.js. Open up the folder in your IDE of choice.', 'We‚Äôll quickly download a couple libraries to help us out.', 'If using Python, open up a terminal and run pip install requests and pip install beautifulsoup4 .', 'If using JavaScript, run npm init and npm install cheerio .', 'For TypeScript specifically, you should run npm install ts-node which will allow you to run the scraper later.', 'Choose a website to scrape and make a network request to the website. For this guide, we‚Äôll scrape example.com because it‚Äôs very simple and doesn‚Äôt have any blocking or authentication. Higher traffic sites like LinkedIn, Indeed, etc are notoriously difficult to scrape due to sophisticated bot detection.', 'In our code, we‚Äôll make a simple GET request to example.com. The website will send us back its HTML. In this step, we‚Äôll just print the HTML to the console. Later, we‚Äôll parse out specific pieces of it.', 'Python:', 'JavaScript:', 'Go ahead and run your script by pasting either python scraper-python.py or node scraper-javascript.js in your terminal. Here‚Äôs the result you should get from printing the HTML:', 'Nice! You made a network request to the website, and got back the HTML. Now, let‚Äôs parse out specific parts of the page.', 'BeautifulSoup and cheerio are libraries that help us navigate HTML in code. They allow us to pass in certain paths and patterns to get certain snippets of HTML.', 'Let‚Äôs go ahead and capture 3 things from this page: The title, the text, and the ‚ÄúMore information‚Ä¶‚Äù link.', 'We‚Äôll use CSS Selectors to find these elements in the HTML. CSS Selectors are notations used to locate HTML elements, and they are very easy to learn. Here‚Äôs a cheatsheet you can use. The ones we will use for this guide are very simple, but it‚Äôs worth your time to get familiar with more complex ones when scraping real websites. Figuring out the right selector is usually not too hard, and I constantly use Google and ChatGPT to help me come up with good ones.', 'Let‚Äôs capture the title, text, and extract the link from the <a> tag. Add these lines to your code.', 'Python:', 'JavaScript:', 'After adding those lines to your script and running it, the console should print out this text:', 'Congratulations! You have built a web scraper.', 'Becoming proficient at web scraping opens up endless possibilities. Especially with the recent advent of AI, mass data collection is more valuable than ever. It‚Äôs also tons of fun and can be a rewarding hobby!', 'Distraction-free reading. No ads.', 'Organize your knowledge with lists and highlights.', 'Tell your story. Find your audience.', 'Read member-only stories', 'Support writers you read most', 'Earn money for your writing', 'Listen to audio narrations', 'Read offline with the Medium app', '146', '146', '9', \"Hi! I'm a software engineer with early stage startup experience. Check out some of my work at https://joeosborne.me :)\", 'Write a response', 'What are your thoughts?', 'Esteban HD', 'Apr 9, 2024', '7', 'Reply', 'Mayur Koshti', 'Jan 24', \"const title = $('h1').text();const text = $('p').text();const link = $('a').attr('href');console.log(title);console.log(text);console.log(link);\", '2', 'Reply', 'Mohit Singla', 'Oct 24, 2024', '3', 'Reply', 'Joe Osborne', 'Joe Osborne', 'Joe Osborne', 'Joe Osborne', 'In', 'Level Up Coding', 'by', 'Maria Ali', 'Joe Osborne', 'In', 'The Pythoneers', 'by', 'Abdur Rahman', 'Yuma Ueno', 'In', 'Data Science Collective', 'by', 'Mil Hoornaert', 'In', 'LearnAItoprofit.com', 'by', 'Raj Monetix üéØ', 'Help', 'Status', 'About', 'Careers', 'Press', 'Blog', 'Privacy', 'Rules', 'Terms', 'Text to speech'], 'code_blocks': ['scraper-python.py', 'scraper-javascript.js', 'pip install requests', 'pip install beautifulsoup4', 'npm init', 'npm install cheerio', 'npm install ts-node', 'GET', \"# scraper-python.py# To run this script, paste `python scraper-python.py` in the terminalimport requestsfrom bs4 import BeautifulSoupdef scrape():        url = 'https://www.example.com'    response = requests.get(url)    soup = BeautifulSoup(response.text, 'html.parser')    print(soup)if __name__ == '__main__':    scrape()\", \"/** * scraper-javascript.js * To run this script, copy and paste `node scraper-javascript.js` in the terminal */const cheerio = require('cheerio');(async () => {  const url = 'https://www.example.com';  const response = await fetch(url);  const $ = cheerio.load(await response.text());  console.log($.html());})();\", 'python scraper-python.py', 'node scraper-javascript.js', '<!DOCTYPE html><html><head>    <title>Example Domain</title>    <meta charset=\"utf-8\">    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">    <style type=\"text/css\">    body {        background-color: #f0f0f2;        margin: 0;        padding: 0;        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;            }    div {        width: 600px;        margin: 5em auto;        padding: 2em;        background-color: #fdfdff;        border-radius: 0.5em;        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);    }    a:link, a:visited {        color: #38488f;        text-decoration: none;    }    @media (max-width: 700px) {        div {            margin: 0 auto;            width: auto;        }    }    </style>    </head><body><div>    <h1>Example Domain</h1>    <p>This domain is for use in illustrative examples in documents. You may use this    domain in literature without prior coordination or asking for permission.</p>    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p></div></body></html>', 'BeautifulSoup', 'cheerio', '<a>', \"title = soup.select_one('h1').texttext = soup.select_one('p').textlink = soup.select_one('a').get('href')print(title)print(text)print(link)\", \"const title = $('h1').text();const text = $('p').text();const link = $('a').attr('href');console.log(title);console.log(text);console.log(link);\", 'Example DomainThis domain is for use in illustrative examples in documents. You may use this    domain in literature without prior coordination or asking for permission.More information...https://www.iana.org/domains/example', 'Nice stuff', 'Nice debugging.... it is the prefect way to debug code specially when we scrape data from the any source. Thank you for sharing.', 'Thank you so much for the tutorial.']}], 'timestamp': '2025-07-15T14:00:13.667832'}\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-x43c7GO8DF",
        "outputId": "42c87a06-fcfe-4f97-d564-f0e159b635ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query\n",
            "agent_discussion\n",
            "scraping_results\n",
            "timestamp\n"
          ]
        }
      ],
      "source": [
        "for res in result:\n",
        "  print(res)\n",
        "  import time\n",
        "  time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ujOWlhdGT2zm",
        "outputId": "309bba46-f244-471a-8b80-66f3448e8dc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110',\n",
              "  'scraped_at': '2025-07-15T13:59:41.931227',\n",
              "  'title': ['Intro to Web Scraping: Build Your First Scraper in 5 Minutes | by Joe Osborne | Medium'],\n",
              "  'headings': ['Intro to Web Scraping: Build Your First Scraper in 5 Minutes',\n",
              "   'A quick guide to help you build a simple web scraper.',\n",
              "   'Step 1',\n",
              "   'Step 2',\n",
              "   'Step 3',\n",
              "   'Resources',\n",
              "   'Sign up to discover human stories that deepen your understanding of the world.',\n",
              "   'Free',\n",
              "   'Membership',\n",
              "   'Written by Joe Osborne',\n",
              "   'Responses (9)',\n",
              "   'More from Joe Osborne',\n",
              "   'How to Log Every Request and Response in FastAPI',\n",
              "   'üìï A guide on efficiently storing logs for every request and response in your FastAPI app',\n",
              "   'FastAPI with API Key Authentication',\n",
              "   'üìò How to set up your FastAPI app with a simple API key auth system.',\n",
              "   'Easy Web Scraping: Scrape an Entire Website in Under 10 Minutes',\n",
              "   'üìï A simple guide on building and running a web scraper.',\n",
              "   'Guide: How to Build a Customized AI Assistant using OpenAI‚Äôs Assistants API',\n",
              "   'An in-depth guide on how to create an assistant, create vector stores with your own documents, and add custom functions that the assistant‚Ä¶',\n",
              "   'Recommended from Medium',\n",
              "   '5 Python-Powered AI Prompts That Quietly Replace Full-Time Jobs',\n",
              "   'These Python-powered automations quietly outperform entire workflows\\u200a‚Äî\\u200aand you can build them in a weekend.',\n",
              "   'Easy Web Scraping: Scrape an Entire Website in Under 10 Minutes',\n",
              "   'üìï A simple guide on building and running a web scraper.',\n",
              "   '5 Python Automation Projects You Can Build This Weekend (And Actually Use)',\n",
              "   'These projects will boost your resume',\n",
              "   'üíÉ How a Vietnamese Developer Grew an AI App to $20,000/Month in a Few Months Using TikTok',\n",
              "   'Hello! I‚Äôm Yuma Ueno(https://twitter.com/stat_biz).',\n",
              "   'Find and Validate Startup Ideas in Minutes Using This AI Tool (Free)',\n",
              "   'I wish I had found out about this earlier',\n",
              "   'I Built a Side Hustle with AI. Now It Pays Me $800 every month',\n",
              "   '$800/Month from My Laptop (No BS )'],\n",
              "  'paragraphs': ['Sign up',\n",
              "   'Sign in',\n",
              "   'Sign up',\n",
              "   'Sign in',\n",
              "   '1',\n",
              "   'Top highlight',\n",
              "   '146',\n",
              "   '9',\n",
              "   'Listen',\n",
              "   'Share',\n",
              "   'Web scraping is my favorite area of coding. It‚Äôs both incredibly frustrating and extremely rewarding. The best thing about scraping is that the possibilities are endless. There‚Äôs virtually infinite data on the internet, and a lot of it is super easy to collect with a simple scraper.',\n",
              "   'Getting started is very easy. When I began, I had only written a few lines of code in my entire life, but I had the idea to scrape mortgage interest rates to watch their trend in real time. After a quick Google search and about 20 minutes later, I had built my first web scraper. I want to help others do the same!',\n",
              "   'I typically use either Python or JavaScript/TypeScript. Both are great options, it‚Äôs mostly a matter of preference. In this guide I‚Äôll go over very simple examples for both.',\n",
              "   'Prerequisites:',\n",
              "   'That‚Äôs it! Let‚Äôs get started.',\n",
              "   'Create a new folder on your machine and a new .js or .py file inside the folder. Let‚Äôs name them scraper-python.pyand scraper-javascript.js. Open up the folder in your IDE of choice.',\n",
              "   'We‚Äôll quickly download a couple libraries to help us out.',\n",
              "   'If using Python, open up a terminal and run pip install requests and pip install beautifulsoup4 .',\n",
              "   'If using JavaScript, run npm init and npm install cheerio .',\n",
              "   'For TypeScript specifically, you should run npm install ts-node which will allow you to run the scraper later.',\n",
              "   'Choose a website to scrape and make a network request to the website. For this guide, we‚Äôll scrape example.com because it‚Äôs very simple and doesn‚Äôt have any blocking or authentication. Higher traffic sites like LinkedIn, Indeed, etc are notoriously difficult to scrape due to sophisticated bot detection.',\n",
              "   'In our code, we‚Äôll make a simple GET request to example.com. The website will send us back its HTML. In this step, we‚Äôll just print the HTML to the console. Later, we‚Äôll parse out specific pieces of it.',\n",
              "   'Python:',\n",
              "   'JavaScript:',\n",
              "   'Go ahead and run your script by pasting either python scraper-python.py or node scraper-javascript.js in your terminal. Here‚Äôs the result you should get from printing the HTML:',\n",
              "   'Nice! You made a network request to the website, and got back the HTML. Now, let‚Äôs parse out specific parts of the page.',\n",
              "   'BeautifulSoup and cheerio are libraries that help us navigate HTML in code. They allow us to pass in certain paths and patterns to get certain snippets of HTML.',\n",
              "   'Let‚Äôs go ahead and capture 3 things from this page: The title, the text, and the ‚ÄúMore information‚Ä¶‚Äù link.',\n",
              "   'We‚Äôll use CSS Selectors to find these elements in the HTML. CSS Selectors are notations used to locate HTML elements, and they are very easy to learn. Here‚Äôs a cheatsheet you can use. The ones we will use for this guide are very simple, but it‚Äôs worth your time to get familiar with more complex ones when scraping real websites. Figuring out the right selector is usually not too hard, and I constantly use Google and ChatGPT to help me come up with good ones.',\n",
              "   'Let‚Äôs capture the title, text, and extract the link from the <a> tag. Add these lines to your code.',\n",
              "   'Python:',\n",
              "   'JavaScript:',\n",
              "   'After adding those lines to your script and running it, the console should print out this text:',\n",
              "   'Congratulations! You have built a web scraper.',\n",
              "   'Becoming proficient at web scraping opens up endless possibilities. Especially with the recent advent of AI, mass data collection is more valuable than ever. It‚Äôs also tons of fun and can be a rewarding hobby!',\n",
              "   'Distraction-free reading. No ads.',\n",
              "   'Organize your knowledge with lists and highlights.',\n",
              "   'Tell your story. Find your audience.',\n",
              "   'Read member-only stories',\n",
              "   'Support writers you read most',\n",
              "   'Earn money for your writing',\n",
              "   'Listen to audio narrations',\n",
              "   'Read offline with the Medium app',\n",
              "   '146',\n",
              "   '146',\n",
              "   '9',\n",
              "   \"Hi! I'm a software engineer with early stage startup experience. Check out some of my work at https://joeosborne.me :)\",\n",
              "   'Write a response',\n",
              "   'What are your thoughts?',\n",
              "   'Esteban HD',\n",
              "   'Apr 9, 2024',\n",
              "   '7',\n",
              "   'Reply',\n",
              "   'Mayur Koshti',\n",
              "   'Jan 24',\n",
              "   \"const title = $('h1').text();const text = $('p').text();const link = $('a').attr('href');console.log(title);console.log(text);console.log(link);\",\n",
              "   '2',\n",
              "   'Reply',\n",
              "   'Mohit Singla',\n",
              "   'Oct 24, 2024',\n",
              "   '3',\n",
              "   'Reply',\n",
              "   'Joe Osborne',\n",
              "   'Joe Osborne',\n",
              "   'Joe Osborne',\n",
              "   'Joe Osborne',\n",
              "   'In',\n",
              "   'Level Up Coding',\n",
              "   'by',\n",
              "   'Maria Ali',\n",
              "   'Joe Osborne',\n",
              "   'In',\n",
              "   'The Pythoneers',\n",
              "   'by',\n",
              "   'Abdur Rahman',\n",
              "   'Yuma Ueno',\n",
              "   'In',\n",
              "   'Data Science Collective',\n",
              "   'by',\n",
              "   'Mil Hoornaert',\n",
              "   'In',\n",
              "   'LearnAItoprofit.com',\n",
              "   'by',\n",
              "   'Raj Monetix üéØ',\n",
              "   'Help',\n",
              "   'Status',\n",
              "   'About',\n",
              "   'Careers',\n",
              "   'Press',\n",
              "   'Blog',\n",
              "   'Privacy',\n",
              "   'Rules',\n",
              "   'Terms',\n",
              "   'Text to speech'],\n",
              "  'code_blocks': ['scraper-python.py',\n",
              "   'scraper-javascript.js',\n",
              "   'pip install requests',\n",
              "   'pip install beautifulsoup4',\n",
              "   'npm init',\n",
              "   'npm install cheerio',\n",
              "   'npm install ts-node',\n",
              "   'GET',\n",
              "   \"# scraper-python.py# To run this script, paste `python scraper-python.py` in the terminalimport requestsfrom bs4 import BeautifulSoupdef scrape():        url = 'https://www.example.com'    response = requests.get(url)    soup = BeautifulSoup(response.text, 'html.parser')    print(soup)if __name__ == '__main__':    scrape()\",\n",
              "   \"/** * scraper-javascript.js * To run this script, copy and paste `node scraper-javascript.js` in the terminal */const cheerio = require('cheerio');(async () => {  const url = 'https://www.example.com';  const response = await fetch(url);  const $ = cheerio.load(await response.text());  console.log($.html());})();\",\n",
              "   'python scraper-python.py',\n",
              "   'node scraper-javascript.js',\n",
              "   '<!DOCTYPE html><html><head>    <title>Example Domain</title>    <meta charset=\"utf-8\">    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">    <style type=\"text/css\">    body {        background-color: #f0f0f2;        margin: 0;        padding: 0;        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;            }    div {        width: 600px;        margin: 5em auto;        padding: 2em;        background-color: #fdfdff;        border-radius: 0.5em;        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);    }    a:link, a:visited {        color: #38488f;        text-decoration: none;    }    @media (max-width: 700px) {        div {            margin: 0 auto;            width: auto;        }    }    </style>    </head><body><div>    <h1>Example Domain</h1>    <p>This domain is for use in illustrative examples in documents. You may use this    domain in literature without prior coordination or asking for permission.</p>    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p></div></body></html>',\n",
              "   'BeautifulSoup',\n",
              "   'cheerio',\n",
              "   '<a>',\n",
              "   \"title = soup.select_one('h1').texttext = soup.select_one('p').textlink = soup.select_one('a').get('href')print(title)print(text)print(link)\",\n",
              "   \"const title = $('h1').text();const text = $('p').text();const link = $('a').attr('href');console.log(title);console.log(text);console.log(link);\",\n",
              "   'Example DomainThis domain is for use in illustrative examples in documents. You may use this    domain in literature without prior coordination or asking for permission.More information...https://www.iana.org/domains/example',\n",
              "   'Nice stuff',\n",
              "   'Nice debugging.... it is the prefect way to debug code specially when we scrape data from the any source. Thank you for sharing.',\n",
              "   'Thank you so much for the tutorial.']}]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['scraping_results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "hmzLnmApT9A-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in result['scraping_results']:\n",
        "        f.write(json.dumps(item, ensure_ascii=False, indent=2))\n",
        "        f.write(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5DnvWJkiR7pB"
      },
      "outputs": [],
      "source": [
        "df=analyze_results(result['scraping_results'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vkmO8lfUSR1Z",
        "outputId": "888b5644-e3e4-47ab-e038-2fb5638a7167"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110',\n",
              "  'scraped_at': '2025-07-15T13:09:57.982084',\n",
              "  'title': ['Intro to Web Scraping: Build Your First Scraper in 5 Minutes | by Joe Osborne | Medium'],\n",
              "  'headings': ['Intro to Web Scraping: Build Your First Scraper in 5 Minutes',\n",
              "   'A quick guide to help you build a simple web scraper.',\n",
              "   'Step 1',\n",
              "   'Step 2',\n",
              "   'Step 3',\n",
              "   'Resources',\n",
              "   'Written by Joe Osborne',\n",
              "   'Responses (9)',\n",
              "   'More from Joe Osborne',\n",
              "   'How to Log Every Request and Response in FastAPI',\n",
              "   'üìï A guide on efficiently storing logs for every request and response in your FastAPI app',\n",
              "   'FastAPI with API Key Authentication',\n",
              "   'üìò How to set up your FastAPI app with a simple API key auth system.',\n",
              "   'Easy Web Scraping: Scrape an Entire Website in Under 10 Minutes',\n",
              "   'üìï A simple guide on building and running a web scraper.',\n",
              "   'Guide: How to Build a Customized AI Assistant using OpenAI‚Äôs Assistants API',\n",
              "   'An in-depth guide on how to create an assistant, create vector stores with your own documents, and add custom functions that the assistant‚Ä¶',\n",
              "   'Recommended from Medium',\n",
              "   '5 Python-Powered AI Prompts That Quietly Replace Full-Time Jobs',\n",
              "   'These Python-powered automations quietly outperform entire workflows\\u200a‚Äî\\u200aand you can build them in a weekend.',\n",
              "   'Easy Web Scraping: Scrape an Entire Website in Under 10 Minutes',\n",
              "   'üìï A simple guide on building and running a web scraper.',\n",
              "   '5 Python Automation Projects You Can Build This Weekend (And Actually Use)',\n",
              "   'These projects will boost your resume',\n",
              "   'üíÉ How a Vietnamese Developer Grew an AI App to $20,000/Month in a Few Months Using TikTok',\n",
              "   'Hello! I‚Äôm Yuma Ueno(https://twitter.com/stat_biz).',\n",
              "   'Find and Validate Startup Ideas in Minutes Using This AI Tool (Free)',\n",
              "   'I wish I had found out about this earlier',\n",
              "   'I Built a Side Hustle with AI. Now It Pays Me $800 every month',\n",
              "   '$800/Month from My Laptop (No BS )'],\n",
              "  'paragraphs': ['Sign up',\n",
              "   'Sign in',\n",
              "   'Sign up',\n",
              "   'Sign in',\n",
              "   '1',\n",
              "   'Top highlight',\n",
              "   '146',\n",
              "   '9',\n",
              "   'Listen',\n",
              "   'Share',\n",
              "   'Web scraping is my favorite area of coding. It‚Äôs both incredibly frustrating and extremely rewarding. The best thing about scraping is that the possibilities are endless. There‚Äôs virtually infinite data on the internet, and a lot of it is super easy to collect with a simple scraper.',\n",
              "   'Getting started is very easy. When I began, I had only written a few lines of code in my entire life, but I had the idea to scrape mortgage interest rates to watch their trend in real time. After a quick Google search and about 20 minutes later, I had built my first web scraper. I want to help others do the same!',\n",
              "   'I typically use either Python or JavaScript/TypeScript. Both are great options, it‚Äôs mostly a matter of preference. In this guide I‚Äôll go over very simple examples for both.',\n",
              "   'Prerequisites:',\n",
              "   'That‚Äôs it! Let‚Äôs get started.',\n",
              "   'Create a new folder on your machine and a new .js or .py file inside the folder. Let‚Äôs name them scraper-python.pyand scraper-javascript.js. Open up the folder in your IDE of choice.',\n",
              "   'We‚Äôll quickly download a couple libraries to help us out.',\n",
              "   'If using Python, open up a terminal and run pip install requests and pip install beautifulsoup4 .',\n",
              "   'If using JavaScript, run npm init and npm install cheerio .',\n",
              "   'For TypeScript specifically, you should run npm install ts-node which will allow you to run the scraper later.',\n",
              "   'Choose a website to scrape and make a network request to the website. For this guide, we‚Äôll scrape example.com because it‚Äôs very simple and doesn‚Äôt have any blocking or authentication. Higher traffic sites like LinkedIn, Indeed, etc are notoriously difficult to scrape due to sophisticated bot detection.',\n",
              "   'In our code, we‚Äôll make a simple GET request to example.com. The website will send us back its HTML. In this step, we‚Äôll just print the HTML to the console. Later, we‚Äôll parse out specific pieces of it.',\n",
              "   'Python:',\n",
              "   'JavaScript:',\n",
              "   'Go ahead and run your script by pasting either python scraper-python.py or node scraper-javascript.js in your terminal. Here‚Äôs the result you should get from printing the HTML:',\n",
              "   'Nice! You made a network request to the website, and got back the HTML. Now, let‚Äôs parse out specific parts of the page.',\n",
              "   'BeautifulSoup and cheerio are libraries that help us navigate HTML in code. They allow us to pass in certain paths and patterns to get certain snippets of HTML.',\n",
              "   'Let‚Äôs go ahead and capture 3 things from this page: The title, the text, and the ‚ÄúMore information‚Ä¶‚Äù link.',\n",
              "   'We‚Äôll use CSS Selectors to find these elements in the HTML. CSS Selectors are notations used to locate HTML elements, and they are very easy to learn. Here‚Äôs a cheatsheet you can use. The ones we will use for this guide are very simple, but it‚Äôs worth your time to get familiar with more complex ones when scraping real websites. Figuring out the right selector is usually not too hard, and I constantly use Google and ChatGPT to help me come up with good ones.',\n",
              "   'Let‚Äôs capture the title, text, and extract the link from the <a> tag. Add these lines to your code.',\n",
              "   'Python:',\n",
              "   'JavaScript:',\n",
              "   'After adding those lines to your script and running it, the console should print out this text:',\n",
              "   'Congratulations! You have built a web scraper.',\n",
              "   'Becoming proficient at web scraping opens up endless possibilities. Especially with the recent advent of AI, mass data collection is more valuable than ever. It‚Äôs also tons of fun and can be a rewarding hobby!',\n",
              "   '146',\n",
              "   '146',\n",
              "   '9',\n",
              "   \"Hi! I'm a software engineer with early stage startup experience. Check out some of my work at https://joeosborne.me :)\",\n",
              "   'Write a response',\n",
              "   'What are your thoughts?',\n",
              "   'Esteban HD',\n",
              "   'Apr 9, 2024',\n",
              "   '7',\n",
              "   'Reply',\n",
              "   'Mayur Koshti',\n",
              "   'Jan 24',\n",
              "   \"const title = $('h1').text();const text = $('p').text();const link = $('a').attr('href');console.log(title);console.log(text);console.log(link);\",\n",
              "   '2',\n",
              "   'Reply',\n",
              "   'Mohit Singla',\n",
              "   'Oct 24, 2024',\n",
              "   '3',\n",
              "   'Reply',\n",
              "   'Joe Osborne',\n",
              "   'Joe Osborne',\n",
              "   'Joe Osborne',\n",
              "   'Joe Osborne',\n",
              "   'In',\n",
              "   'Level Up Coding',\n",
              "   'by',\n",
              "   'Maria Ali',\n",
              "   'Joe Osborne',\n",
              "   'In',\n",
              "   'The Pythoneers',\n",
              "   'by',\n",
              "   'Abdur Rahman',\n",
              "   'Yuma Ueno',\n",
              "   'In',\n",
              "   'Data Science Collective',\n",
              "   'by',\n",
              "   'Mil Hoornaert',\n",
              "   'In',\n",
              "   'LearnAItoprofit.com',\n",
              "   'by',\n",
              "   'Raj Monetix üéØ',\n",
              "   'Help',\n",
              "   'Status',\n",
              "   'About',\n",
              "   'Careers',\n",
              "   'Press',\n",
              "   'Blog',\n",
              "   'Privacy',\n",
              "   'Rules',\n",
              "   'Terms',\n",
              "   'Text to speech']}]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['scraping_results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9TmJ8_CBIrwq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "def analyze_results(results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    \"\"\"Convert scraping results to DataFrame including raw content\"\"\"\n",
        "\n",
        "    if not results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    flattened = []\n",
        "    for result in results:\n",
        "        if 'error' not in result:\n",
        "            flattened.append({\n",
        "                'url': result.get('url', ''),\n",
        "                'scraped_at': result.get('scraped_at', ''),\n",
        "                'title_count': len(result.get('title', [])),\n",
        "                'headings_count': len(result.get('headings', [])),\n",
        "                'paragraphs_count': len(result.get('paragraphs', [])),\n",
        "                'links_count': len(result.get('links', [])) if 'links' in result else 0,\n",
        "                # Join lists into plain text strings for CSV\n",
        "                'title': \" | \".join(result.get('title', [])),\n",
        "                'headings': \" | \".join(result.get('headings', [])),\n",
        "                'paragraphs': \" | \".join(result.get('paragraphs', []))\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(flattened)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-TJELP4JFT02"
      },
      "outputs": [],
      "source": [
        "# Assume `scraped_data` is your list of dicts\n",
        "df = analyze_results(result['scraping_results'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOzgqe-PFTxY",
        "outputId": "c983062d-cd1b-4617-e775-b3e6eb4ca5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CSV file saved as 'scraping_analysis.csv'\n"
          ]
        }
      ],
      "source": [
        "# Export to CSV\n",
        "df.to_csv('scraping_analysis.csv', index=False)\n",
        "print(\"‚úÖ CSV file saved as 'scraping_analysis.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "IDiKXUKFFTvC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Q2L-HSmuTIDn",
        "outputId": "d23a20a7-9dea-4d91-aa12-886f99bf2f66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"headings_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paragraphs_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 86,\n        \"max\": 86,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          86\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"links_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"scraped_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-07-15T13:09:57.982084\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ed8684aa-2a86-4041-a812-15d008edf166\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title_count</th>\n",
              "      <th>headings_count</th>\n",
              "      <th>paragraphs_count</th>\n",
              "      <th>links_count</th>\n",
              "      <th>scraped_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://medium.com/@joerosborne/intro-to-web-s...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>2025-07-15T13:09:57.982084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed8684aa-2a86-4041-a812-15d008edf166')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed8684aa-2a86-4041-a812-15d008edf166 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed8684aa-2a86-4041-a812-15d008edf166');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                 url  title_count  \\\n",
              "0  https://medium.com/@joerosborne/intro-to-web-s...            1   \n",
              "\n",
              "   headings_count  paragraphs_count  links_count                  scraped_at  \n",
              "0              30                86            0  2025-07-15T13:09:57.982084  "
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_csv('/content/scraping_analysis.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
