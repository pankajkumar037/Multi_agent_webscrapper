{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P27hepTolWWd"
      },
      "source": [
        "###Just Dial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blBbQoBtlVmR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_in1rx6FX46",
        "outputId": "d54f2185-dd7d-479e-bd01-47df5494190d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autogen-agentchat\n",
            "  Downloading autogen_agentchat-0.6.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting autogen-core==0.6.4 (from autogen-agentchat)\n",
            "  Downloading autogen_core-0.6.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core==0.6.4->autogen-agentchat)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting opentelemetry-api>=1.34.1 (from autogen-core==0.6.4->autogen-agentchat)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (11.2.1)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat) (4.14.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.34.1->autogen-core==0.6.4->autogen-agentchat) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.4->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.4->autogen-agentchat) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.4->autogen-agentchat) (0.4.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.6.4->autogen-agentchat) (3.23.0)\n",
            "Downloading autogen_agentchat-0.6.4-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogen_core-0.6.4-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jsonref, opentelemetry-api, autogen-core, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.6.4 autogen-core-0.6.4 jsonref-1.1.0 opentelemetry-api-1.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen-agentchat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQTnUGeWTK5H",
        "outputId": "f675cd20-621d-4111-b17d-a144beb43e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<14,>=13->playwright) (4.14.1)\n",
            "Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl (45.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.53.0 pyee-13.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install playwright"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y_v3Lb-FX1W",
        "outputId": "0aff2e4f-3b89-4d92-fe0f-63f9624620ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Chromium 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G171.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 0% 61.6s\u001b[0K\u001b[1G171.6 MiB [] 0% 27.0s\u001b[0K\u001b[1G171.6 MiB [] 0% 13.7s\u001b[0K\u001b[1G171.6 MiB [] 0% 8.7s\u001b[0K\u001b[1G171.6 MiB [] 1% 7.1s\u001b[0K\u001b[1G171.6 MiB [] 1% 6.2s\u001b[0K\u001b[1G171.6 MiB [] 2% 5.8s\u001b[0K\u001b[1G171.6 MiB [] 2% 5.5s\u001b[0K\u001b[1G171.6 MiB [] 3% 4.8s\u001b[0K\u001b[1G171.6 MiB [] 3% 4.0s\u001b[0K\u001b[1G171.6 MiB [] 4% 3.7s\u001b[0K\u001b[1G171.6 MiB [] 5% 3.6s\u001b[0K\u001b[1G171.6 MiB [] 6% 3.5s\u001b[0K\u001b[1G171.6 MiB [] 7% 3.1s\u001b[0K\u001b[1G171.6 MiB [] 8% 2.9s\u001b[0K\u001b[1G171.6 MiB [] 9% 2.8s\u001b[0K\u001b[1G171.6 MiB [] 10% 2.6s\u001b[0K\u001b[1G171.6 MiB [] 11% 2.5s\u001b[0K\u001b[1G171.6 MiB [] 12% 2.4s\u001b[0K\u001b[1G171.6 MiB [] 13% 2.5s\u001b[0K\u001b[1G171.6 MiB [] 13% 2.4s\u001b[0K\u001b[1G171.6 MiB [] 14% 2.4s\u001b[0K\u001b[1G171.6 MiB [] 15% 2.3s\u001b[0K\u001b[1G171.6 MiB [] 16% 2.2s\u001b[0K\u001b[1G171.6 MiB [] 17% 2.1s\u001b[0K\u001b[1G171.6 MiB [] 18% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 20% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 21% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 21% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 22% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 24% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 25% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 26% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 27% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 29% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 30% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 32% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 33% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 34% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 35% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 37% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 38% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 40% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 41% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 42% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 43% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 45% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 46% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 48% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 49% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 50% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 52% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 53% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 55% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 56% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 57% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 59% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 60% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 61% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 63% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 64% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 65% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 66% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 67% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 68% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 69% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 70% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 71% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 72% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 73% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 74% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 75% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 76% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 77% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 78% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 79% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 80% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 81% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 82% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 83% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 83% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 84% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 85% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 86% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 87% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 88% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 89% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 90% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 91% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 92% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 95% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 97% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 98% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium-1179\n",
            "Downloading Chromium Headless Shell 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 0% 47.1s\u001b[0K\u001b[1G104.5 MiB [] 0% 30.1s\u001b[0K\u001b[1G104.5 MiB [] 0% 13.8s\u001b[0K\u001b[1G104.5 MiB [] 1% 7.9s\u001b[0K\u001b[1G104.5 MiB [] 1% 7.2s\u001b[0K\u001b[1G104.5 MiB [] 2% 5.9s\u001b[0K\u001b[1G104.5 MiB [] 2% 6.1s\u001b[0K\u001b[1G104.5 MiB [] 2% 6.5s\u001b[0K\u001b[1G104.5 MiB [] 2% 6.6s\u001b[0K\u001b[1G104.5 MiB [] 3% 6.3s\u001b[0K\u001b[1G104.5 MiB [] 3% 6.1s\u001b[0K\u001b[1G104.5 MiB [] 4% 6.0s\u001b[0K\u001b[1G104.5 MiB [] 4% 5.7s\u001b[0K\u001b[1G104.5 MiB [] 5% 5.7s\u001b[0K\u001b[1G104.5 MiB [] 5% 5.4s\u001b[0K\u001b[1G104.5 MiB [] 6% 5.2s\u001b[0K\u001b[1G104.5 MiB [] 6% 5.4s\u001b[0K\u001b[1G104.5 MiB [] 6% 5.2s\u001b[0K\u001b[1G104.5 MiB [] 7% 5.1s\u001b[0K\u001b[1G104.5 MiB [] 7% 4.8s\u001b[0K\u001b[1G104.5 MiB [] 8% 4.7s\u001b[0K\u001b[1G104.5 MiB [] 8% 4.6s\u001b[0K\u001b[1G104.5 MiB [] 9% 4.5s\u001b[0K\u001b[1G104.5 MiB [] 9% 4.4s\u001b[0K\u001b[1G104.5 MiB [] 9% 4.6s\u001b[0K\u001b[1G104.5 MiB [] 10% 4.4s\u001b[0K\u001b[1G104.5 MiB [] 11% 4.1s\u001b[0K\u001b[1G104.5 MiB [] 12% 3.9s\u001b[0K\u001b[1G104.5 MiB [] 13% 3.8s\u001b[0K\u001b[1G104.5 MiB [] 14% 3.7s\u001b[0K\u001b[1G104.5 MiB [] 15% 3.6s\u001b[0K\u001b[1G104.5 MiB [] 15% 3.5s\u001b[0K\u001b[1G104.5 MiB [] 16% 3.5s\u001b[0K\u001b[1G104.5 MiB [] 17% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 18% 3.1s\u001b[0K\u001b[1G104.5 MiB [] 20% 2.8s\u001b[0K\u001b[1G104.5 MiB [] 21% 2.8s\u001b[0K\u001b[1G104.5 MiB [] 22% 2.7s\u001b[0K\u001b[1G104.5 MiB [] 23% 2.6s\u001b[0K\u001b[1G104.5 MiB [] 25% 2.4s\u001b[0K\u001b[1G104.5 MiB [] 26% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 27% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 28% 2.3s\u001b[0K\u001b[1G104.5 MiB [] 29% 2.2s\u001b[0K\u001b[1G104.5 MiB [] 30% 2.2s\u001b[0K\u001b[1G104.5 MiB [] 32% 2.1s\u001b[0K\u001b[1G104.5 MiB [] 34% 2.0s\u001b[0K\u001b[1G104.5 MiB [] 35% 1.9s\u001b[0K\u001b[1G104.5 MiB [] 36% 1.8s\u001b[0K\u001b[1G104.5 MiB [] 37% 1.8s\u001b[0K\u001b[1G104.5 MiB [] 39% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 40% 1.6s\u001b[0K\u001b[1G104.5 MiB [] 41% 1.6s\u001b[0K\u001b[1G104.5 MiB [] 42% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 42% 1.6s\u001b[0K\u001b[1G104.5 MiB [] 44% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 45% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 46% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 47% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 48% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 49% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 50% 1.4s\u001b[0K\u001b[1G104.5 MiB [] 51% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 53% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 54% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 55% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 56% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 57% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 58% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 59% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 60% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 61% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 62% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 63% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 64% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 65% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 66% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 67% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 69% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 70% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 71% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 73% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 74% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 76% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 77% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 78% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 80% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 80% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 82% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 83% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 85% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 86% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 88% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 89% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 91% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 93% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 97% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1179\n",
            "Downloading Firefox 139.0 (playwright build v1488)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1488/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G92.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 0% 11.8s\u001b[0K\u001b[1G92.3 MiB [] 0% 8.1s\u001b[0K\u001b[1G92.3 MiB [] 1% 3.2s\u001b[0K\u001b[1G92.3 MiB [] 2% 2.4s\u001b[0K\u001b[1G92.3 MiB [] 3% 2.2s\u001b[0K\u001b[1G92.3 MiB [] 4% 2.2s\u001b[0K\u001b[1G92.3 MiB [] 5% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 6% 1.8s\u001b[0K\u001b[1G92.3 MiB [] 6% 2.1s\u001b[0K\u001b[1G92.3 MiB [] 7% 2.0s\u001b[0K\u001b[1G92.3 MiB [] 8% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 9% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 10% 1.8s\u001b[0K\u001b[1G92.3 MiB [] 10% 2.0s\u001b[0K\u001b[1G92.3 MiB [] 11% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 12% 1.9s\u001b[0K\u001b[1G92.3 MiB [] 13% 1.8s\u001b[0K\u001b[1G92.3 MiB [] 15% 1.7s\u001b[0K\u001b[1G92.3 MiB [] 16% 1.6s\u001b[0K\u001b[1G92.3 MiB [] 17% 1.6s\u001b[0K\u001b[1G92.3 MiB [] 18% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 20% 1.4s\u001b[0K\u001b[1G92.3 MiB [] 22% 1.3s\u001b[0K\u001b[1G92.3 MiB [] 24% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 26% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 28% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 30% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 32% 1.0s\u001b[0K\u001b[1G92.3 MiB [] 34% 0.9s\u001b[0K\u001b[1G92.3 MiB [] 36% 0.9s\u001b[0K\u001b[1G92.3 MiB [] 38% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 39% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 41% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 43% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 45% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 47% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 49% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 49% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 51% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 53% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 55% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 57% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 59% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 61% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 63% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 65% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 67% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 69% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 72% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 73% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 76% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 78% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 80% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 82% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 84% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 86% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 87% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 89% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 94% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 96% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 139.0 (playwright build v1488) downloaded to /root/.cache/ms-playwright/firefox-1488\n",
            "Downloading Webkit 18.5 (playwright build v2182)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2182/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G93.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 0% 9.7s\u001b[0K\u001b[1G93.7 MiB [] 0% 5.3s\u001b[0K\u001b[1G93.7 MiB [] 1% 3.9s\u001b[0K\u001b[1G93.7 MiB [] 2% 3.4s\u001b[0K\u001b[1G93.7 MiB [] 2% 3.0s\u001b[0K\u001b[1G93.7 MiB [] 3% 2.8s\u001b[0K\u001b[1G93.7 MiB [] 4% 2.5s\u001b[0K\u001b[1G93.7 MiB [] 6% 2.2s\u001b[0K\u001b[1G93.7 MiB [] 7% 2.0s\u001b[0K\u001b[1G93.7 MiB [] 8% 1.9s\u001b[0K\u001b[1G93.7 MiB [] 9% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 10% 2.0s\u001b[0K\u001b[1G93.7 MiB [] 11% 2.0s\u001b[0K\u001b[1G93.7 MiB [] 12% 2.0s\u001b[0K\u001b[1G93.7 MiB [] 14% 1.9s\u001b[0K\u001b[1G93.7 MiB [] 15% 1.9s\u001b[0K\u001b[1G93.7 MiB [] 16% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 17% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 18% 1.9s\u001b[0K\u001b[1G93.7 MiB [] 19% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 20% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 21% 1.7s\u001b[0K\u001b[1G93.7 MiB [] 23% 1.7s\u001b[0K\u001b[1G93.7 MiB [] 24% 1.6s\u001b[0K\u001b[1G93.7 MiB [] 24% 1.7s\u001b[0K\u001b[1G93.7 MiB [] 25% 1.7s\u001b[0K\u001b[1G93.7 MiB [] 26% 1.7s\u001b[0K\u001b[1G93.7 MiB [] 27% 1.6s\u001b[0K\u001b[1G93.7 MiB [] 28% 1.6s\u001b[0K\u001b[1G93.7 MiB [] 29% 1.6s\u001b[0K\u001b[1G93.7 MiB [] 30% 1.5s\u001b[0K\u001b[1G93.7 MiB [] 31% 1.5s\u001b[0K\u001b[1G93.7 MiB [] 32% 1.5s\u001b[0K\u001b[1G93.7 MiB [] 33% 1.4s\u001b[0K\u001b[1G93.7 MiB [] 34% 1.4s\u001b[0K\u001b[1G93.7 MiB [] 36% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 37% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 38% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 39% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 40% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 41% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 42% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 43% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 44% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 45% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 46% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 48% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 49% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 49% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 49% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 50% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 52% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 53% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 54% 0.9s\u001b[0K\u001b[1G93.7 MiB [] 55% 0.9s\u001b[0K\u001b[1G93.7 MiB [] 57% 0.9s\u001b[0K\u001b[1G93.7 MiB [] 59% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 60% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 61% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 63% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 65% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 66% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 68% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 69% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 71% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 72% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 73% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 74% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 75% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 76% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 77% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 78% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 79% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 80% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 81% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 83% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 84% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 85% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 87% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 88% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.5 (playwright build v2182) downloaded to /root/.cache/ms-playwright/webkit-2182\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 15% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 47% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë Host system is missing dependencies to run browsers. ‚ïë\n",
            "‚ïë Missing libraries:                                   ‚ïë\n",
            "‚ïë     libwoff2dec.so.1.0.2                             ‚ïë\n",
            "‚ïë     libgstgl-1.0.so.0                                ‚ïë\n",
            "‚ïë     libgstcodecparsers-1.0.so.0                      ‚ïë\n",
            "‚ïë     libavif.so.13                                    ‚ïë\n",
            "‚ïë     libharfbuzz-icu.so.0                             ‚ïë\n",
            "‚ïë     libenchant-2.so.2                                ‚ïë\n",
            "‚ïë     libsecret-1.so.0                                 ‚ïë\n",
            "‚ïë     libhyphen.so.0                                   ‚ïë\n",
            "‚ïë     libmanette-0.2.so.0                              ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:927:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1049:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1038:7)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ],
      "source": [
        "!python -m playwright install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSW3WddSFXy3",
        "outputId": "aac7b1d5-4b07-4c9f-f311-09c9a94509db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install  beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg39gKERGBmk",
        "outputId": "202c6fd2-bf28-4955-98b1-54c01644950b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromium\n",
            "  Downloading chromium-0.0.0-py3-none-any.whl.metadata (615 bytes)\n",
            "Downloading chromium-0.0.0-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: chromium\n",
            "Successfully installed chromium-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install  chromium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGZT1MNUGbVH",
        "outputId": "29869ea1-ffe6-401e-c2f9-df48fac260ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.9.6-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ag2==0.9.6 (from autogen)\n",
            "  Downloading ag2-0.9.6-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from ag2==0.9.6->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from ag2==0.9.6->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from ag2==0.9.6->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (2.11.7)\n",
            "Collecting python-dotenv (from ag2==0.9.6->autogen)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (0.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (4.14.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.6->autogen) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.6->autogen) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.9.6->autogen) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.6->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.6->autogen) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.6->autogen) (0.4.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->ag2==0.9.6->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->ag2==0.9.6->autogen) (2.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ag2==0.9.6->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->ag2==0.9.6->autogen) (3.4.2)\n",
            "Downloading autogen-0.9.6-py3-none-any.whl (13 kB)\n",
            "Downloading ag2-0.9.6-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m859.2/859.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, diskcache, docker, asyncer, ag2, autogen\n",
            "Successfully installed ag2-0.9.6 asyncer-0.0.8 autogen-0.9.6 diskcache-5.6.3 docker-7.1.0 python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLFvAhC6VO6d"
      },
      "source": [
        "###**Multipage Scrapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-h7h-02WQEY"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "import autogen\n",
        "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# Configuration\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"api_key\": your_api_key,  # Replace with your actual API key\n",
        "        \"base_url\": \"https://api.openai.com/v1\",\n",
        "    }\n",
        "]\n",
        "\n",
        "llm_config = {\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0.1,\n",
        "    \"timeout\": 120,\n",
        "}\n",
        "\n",
        "class SimpleWebScraper:\n",
        "    \"\"\"Simple web scraper using Playwright with multi-page support\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.browser = None\n",
        "        self.context = None\n",
        "        self.results = []\n",
        "        self.scraped_urls = set()  # Track scraped URLs to avoid duplicates\n",
        "\n",
        "    async def setup_browser(self, headless=True):\n",
        "        \"\"\"Initialize browser\"\"\"\n",
        "        playwright = await async_playwright().start()\n",
        "        self.browser = await playwright.chromium.launch(headless=headless)\n",
        "        self.context = await self.browser.new_context(\n",
        "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "            viewport={\"width\": 1920, \"height\": 1080}\n",
        "        )\n",
        "\n",
        "        # Add request interception to handle anti-bot measures\n",
        "        await self.context.route(\"**/*\", self.handle_request)\n",
        "\n",
        "    async def handle_request(self, route):\n",
        "        \"\"\"Handle requests to avoid detection\"\"\"\n",
        "        await route.continue_()\n",
        "\n",
        "    async def wait_for_page_load(self, page):\n",
        "        \"\"\"Wait for page to fully load\"\"\"\n",
        "        try:\n",
        "            await page.wait_for_load_state('networkidle', timeout=10000)\n",
        "            await asyncio.sleep(2)  # Additional wait for dynamic content\n",
        "        except:\n",
        "            await asyncio.sleep(3)  # Fallback wait\n",
        "\n",
        "    async def extract_pagination_links(self, page, base_url):\n",
        "        \"\"\"Extract pagination links from current page\"\"\"\n",
        "        pagination_links = []\n",
        "\n",
        "        # Common pagination selectors for Just Dial and similar sites\n",
        "        pagination_selectors = [\n",
        "            'a[href*=\"page\"]',\n",
        "            'a[href*=\"pn-\"]',\n",
        "            '.pagination a',\n",
        "            '.pager a',\n",
        "            'a.next',\n",
        "            'a[aria-label*=\"Next\"]',\n",
        "            'a[title*=\"Next\"]',\n",
        "            'a[href*=\"city=\"]',\n",
        "            'a.pageNext',\n",
        "            '.page-numbers a',\n",
        "             \"li.next a\",\n",
        "        ]\n",
        "\n",
        "        for selector in pagination_selectors:\n",
        "            try:\n",
        "                elements = await page.query_selector_all(selector)\n",
        "                for element in elements:\n",
        "                    href = await element.get_attribute('href')\n",
        "                    if href and href not in self.scraped_urls:\n",
        "                        # Convert relative URLs to absolute\n",
        "                        if href.startswith('/'):\n",
        "                            href = urljoin(base_url, href)\n",
        "                        elif not href.startswith('http'):\n",
        "                            href = urljoin(base_url, href)\n",
        "\n",
        "                        pagination_links.append(href)\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting pagination from {selector}: {e}\")\n",
        "\n",
        "        return list(set(pagination_links))  # Remove duplicates\n",
        "\n",
        "    async def scrape_page(self, url: str, selectors: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Scrape a single page\"\"\"\n",
        "        try:\n",
        "            page = await self.context.new_page()\n",
        "            await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
        "            await self.wait_for_page_load(page)\n",
        "\n",
        "            data = {\"url\": url, \"scraped_at\": datetime.now().isoformat()}\n",
        "\n",
        "            # Enhanced selectors for Just Dial type sites\n",
        "            enhanced_selectors = {\n",
        "                \"business_name\": [\n",
        "                    \".jcn a\",\n",
        "                    \".resultbox .rsltTitle\",\n",
        "                    \".store-name\",\n",
        "                    \".business-name\",\n",
        "                    \"h3.fn\",\n",
        "                    \".comp-name\",\n",
        "                    \".listing-title\"\n",
        "                ],\n",
        "                \"phone_number\": [\n",
        "                    \".contact-info .mobile\",\n",
        "                    \".ph\",\n",
        "                    \".phoneNumber\",\n",
        "                    \".tel\",\n",
        "                    \"a[href*='tel:']\",\n",
        "                    \".contact-number\",\n",
        "                    \".phone\"\n",
        "                ],\n",
        "                \"address\": [\n",
        "                    \".address\",\n",
        "                    \".addr\",\n",
        "                    \".location\",\n",
        "                    \".adr\",\n",
        "                    \".locality\",\n",
        "                    \".business-address\"\n",
        "                ],\n",
        "                \"image\": [\n",
        "                    \".resultbox img\",\n",
        "                    \".store-image img\",\n",
        "                    \".business-logo img\",\n",
        "                    \".listing-image img\",\n",
        "                    \".photo img\"\n",
        "                ],\n",
        "                \"rating\": [\n",
        "                    \".rating\",\n",
        "                    \".star-rating\",\n",
        "                    \".review-rating\",\n",
        "                    \".rating-value\"\n",
        "                ],\n",
        "                \"reviews\": [\n",
        "                    \".review-count\",\n",
        "                    \".total-reviews\",\n",
        "                    \".reviews-count\"\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            # Use enhanced selectors if available, otherwise use provided selectors\n",
        "            for key, selector in selectors.items():\n",
        "                try:\n",
        "                    if key in enhanced_selectors:\n",
        "                        # Try enhanced selectors first\n",
        "                        elements = []\n",
        "                        for enhanced_selector in enhanced_selectors[key]:\n",
        "                            try:\n",
        "                                found_elements = await page.query_selector_all(enhanced_selector)\n",
        "                                if found_elements:\n",
        "                                    elements.extend(found_elements)\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                        # If no enhanced selectors work, use the provided selector\n",
        "                        if not elements:\n",
        "                            elements = await page.query_selector_all(selector)\n",
        "                    else:\n",
        "                        elements = await page.query_selector_all(selector)\n",
        "\n",
        "                    # Extract data based on element type\n",
        "                    if key == \"image\":\n",
        "                        data[key] = []\n",
        "                        for el in elements:\n",
        "                            src = await el.get_attribute('src')\n",
        "                            if src:\n",
        "                                data[key].append(src)\n",
        "                    elif key == \"phone_number\":\n",
        "                        data[key] = []\n",
        "                        for el in elements:\n",
        "                            text = await el.text_content()\n",
        "                            if text:\n",
        "                                # Extract phone numbers using regex\n",
        "                                phone_matches = re.findall(r'[\\d\\s\\-\\+\\(\\)]{8,}', text.strip())\n",
        "                                data[key].extend(phone_matches)\n",
        "                    else:\n",
        "                        data[key] = []\n",
        "                        for el in elements:\n",
        "                            text = await el.text_content()\n",
        "                            if text and text.strip():\n",
        "                                data[key].append(text.strip())\n",
        "\n",
        "                except Exception as e:\n",
        "                    data[key] = []\n",
        "                    print(f\"Error scraping {key}: {e}\")\n",
        "\n",
        "            # Get pagination links\n",
        "            pagination_links = await self.extract_pagination_links(page, url)\n",
        "            data[\"pagination_links\"] = pagination_links\n",
        "\n",
        "            await page.close()\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            return {\"url\": url, \"error\": str(e)}\n",
        "\n",
        "    async def scrape_multiple_pages(self, start_url: str, selectors: Dict[str, str], max_pages: int = 10) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Scrape multiple pages with pagination\"\"\"\n",
        "        results = []\n",
        "        urls_to_scrape = [start_url]\n",
        "        pages_scraped = 0\n",
        "\n",
        "        while urls_to_scrape and pages_scraped < max_pages:\n",
        "            current_url = urls_to_scrape.pop(0)\n",
        "\n",
        "            if current_url in self.scraped_urls:\n",
        "                continue\n",
        "\n",
        "            print(f\"üìÑ Scraping page {pages_scraped + 1}: {current_url}\")\n",
        "\n",
        "            # Scrape current page\n",
        "            page_data = await self.scrape_page(current_url, selectors)\n",
        "            results.append(page_data)\n",
        "\n",
        "            self.scraped_urls.add(current_url)\n",
        "            pages_scraped += 1\n",
        "\n",
        "            # Add pagination links to queue\n",
        "            if \"pagination_links\" in page_data:\n",
        "                for link in page_data[\"pagination_links\"]:\n",
        "                    if link not in self.scraped_urls and link not in urls_to_scrape:\n",
        "                        urls_to_scrape.append(link)\n",
        "\n",
        "            # Add delay between requests\n",
        "            await asyncio.sleep(2)\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def close(self):\n",
        "        \"\"\"Close browser\"\"\"\n",
        "        if self.browser:\n",
        "            await self.browser.close()\n",
        "\n",
        "# Global scraper instance\n",
        "scraper = SimpleWebScraper()\n",
        "\n",
        "class MultiAgentWebScrapingSystem:\n",
        "    \"\"\"5-Agent Web Scraping System with Multi-Page Support\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.results = []\n",
        "        self.session_data = {}\n",
        "        self.setup_agents()\n",
        "\n",
        "    def setup_agents(self):\n",
        "        \"\"\"Create the 5 specialized agents\"\"\"\n",
        "\n",
        "        # 1. Master Coordinator Agent\n",
        "        self.coordinator = AssistantAgent(\n",
        "            name=\"MasterCoordinator\",\n",
        "            system_message=\"\"\"You are the Master Coordinator Agent. Your role is to:\n",
        "            1. Analyze user queries and understand what data needs to be scraped\n",
        "            2. Coordinate with other agents to plan and execute multi-page scraping tasks\n",
        "            3. Compile and present final results from all pages\n",
        "            4. Always start by asking the SearchStrategy agent to analyze the query\n",
        "            5. Distribute tasks to appropriate scraper agents\n",
        "            6. Summarize results from all agents and pages\n",
        "            7. Handle pagination strategy and ensure comprehensive data collection\n",
        "\n",
        "            Format your responses clearly and coordinate the workflow efficiently.\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 2. Search Strategy Agent\n",
        "        self.strategy_agent = AssistantAgent(\n",
        "            name=\"SearchStrategy\",\n",
        "            system_message=\"\"\"You are the Search Strategy Agent. Your role is to:\n",
        "            1. Analyze user queries to understand intent and extract keywords\n",
        "            2. Identify relevant websites and data sources for scraping\n",
        "            3. Generate specific scraping strategies for different types of content\n",
        "            4. Recommend which specialized scraper agents should handle each task\n",
        "            5. Suggest CSS selectors and scraping patterns for multi-page scraping\n",
        "            6. Plan pagination strategies for sites like Just Dial\n",
        "\n",
        "            Provide detailed scraping plans with:\n",
        "            - Target websites\n",
        "            - Recommended selectors for business data (name, phone, address, images)\n",
        "            - Data extraction strategies\n",
        "            - Pagination handling approach\n",
        "            - Which scraper agent to use\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 3. Generic Web Scraper Agent\n",
        "        self.generic_scraper = AssistantAgent(\n",
        "            name=\"GenericScraper\",\n",
        "            system_message=\"\"\"You are the Generic Web Scraper Agent. Your role is to:\n",
        "            1. Handle general-purpose web scraping tasks across multiple pages\n",
        "            2. Extract content from various website types including business directories\n",
        "            3. Use Playwright to navigate and scrape web pages with pagination\n",
        "            4. Handle dynamic content and anti-bot measures\n",
        "            5. Report scraping results and any issues encountered\n",
        "\n",
        "            When given a scraping task, provide:\n",
        "            - Confirmation of the task received\n",
        "            - Scraping approach and selectors to use\n",
        "            - Multi-page scraping strategy\n",
        "            - Results summary after scraping\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 4. Data Processing Agent\n",
        "        self.data_processor = AssistantAgent(\n",
        "            name=\"DataProcessor\",\n",
        "            system_message=\"\"\"You are the Data Processing Agent. Your role is to:\n",
        "            1. Clean and structure scraped data from multiple pages\n",
        "            2. Remove duplicates and invalid entries across all pages\n",
        "            3. Standardize data formats (especially phone numbers, addresses)\n",
        "            4. Organize data into structured formats\n",
        "            5. Perform basic data validation\n",
        "            6. Merge data from multiple pages efficiently\n",
        "\n",
        "            When processing data, provide:\n",
        "            - Data cleaning summary\n",
        "            - Structure improvements made\n",
        "            - Duplicate removal statistics\n",
        "            - Final processed data format\n",
        "            - Data quality assessment\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # 5. Quality Assurance Agent\n",
        "        self.qa_agent = AssistantAgent(\n",
        "            name=\"QualityAssurance\",\n",
        "            system_message=\"\"\"You are the Quality Assurance Agent. Your role is to:\n",
        "            1. Verify data accuracy and completeness across all scraped pages\n",
        "            2. Check for inconsistencies across scraped data\n",
        "            3. Validate data quality metrics\n",
        "            4. Flag potential errors or issues\n",
        "            5. Provide quality scores and recommendations\n",
        "            6. Ensure pagination worked correctly and no data was missed\n",
        "\n",
        "            When reviewing data, provide:\n",
        "            - Quality assessment report\n",
        "            - Identified issues and inconsistencies\n",
        "            - Recommendations for improvement\n",
        "            - Coverage analysis (pages scraped vs expected)\n",
        "            - Overall quality score (1-10)\"\"\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "        # User proxy for human interaction\n",
        "        self.user_proxy = UserProxyAgent(\n",
        "            name=\"UserProxy\",\n",
        "            system_message=\"You facilitate communication between the user and the agent system.\",\n",
        "            code_execution_config={\"work_dir\": \"scraping_work\", \"use_docker\": False},\n",
        "            human_input_mode=\"NEVER\"\n",
        "        )\n",
        "\n",
        "        # Create group chat\n",
        "        self.agents = [\n",
        "            self.coordinator,\n",
        "            self.strategy_agent,\n",
        "            self.generic_scraper,\n",
        "            self.data_processor,\n",
        "            self.qa_agent\n",
        "        ]\n",
        "\n",
        "        self.groupchat = GroupChat(\n",
        "            agents=self.agents,\n",
        "            messages=[],\n",
        "            max_round=25,  # Increased rounds for multi-page processing\n",
        "            speaker_selection_method=\"auto\"  # Changed to auto for better flow\n",
        "        )\n",
        "\n",
        "        self.manager = GroupChatManager(\n",
        "            groupchat=self.groupchat,\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "\n",
        "    async def execute_scraping_task(self, query: str):\n",
        "        \"\"\"Execute the multi-agent scraping workflow\"\"\"\n",
        "\n",
        "        print(f\"üöÄ Starting multi-agent scraping for query: '{query}'\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Start the conversation\n",
        "        initial_message = f\"\"\"\n",
        "        New multi-page scraping request: \"{query}\"\n",
        "\n",
        "        Please coordinate to:\n",
        "        1. Analyze the query and determine scraping strategy\n",
        "        2. Identify target websites and data sources\n",
        "        3. Plan multi-page scraping approach\n",
        "        4. Execute scraping tasks across multiple pages\n",
        "        5. Process and clean the data from all pages\n",
        "        6. Perform quality assurance on complete dataset\n",
        "        7. Provide final consolidated results\n",
        "\n",
        "        Let's begin with the SearchStrategy agent analyzing this query.\n",
        "        \"\"\"\n",
        "\n",
        "        # Execute the group chat\n",
        "        await self.user_proxy.a_initiate_chat(\n",
        "            self.manager,\n",
        "            message=initial_message\n",
        "        )\n",
        "\n",
        "        return self.groupchat.messages\n",
        "\n",
        "    async def scrape_with_agents(self, query: str, urls: List[str] = None, selectors: Dict[str, str] = None, max_pages: int = 10):\n",
        "        \"\"\"Enhanced scraping function with multi-page support integrated with agents\"\"\"\n",
        "\n",
        "        if not urls:\n",
        "            # Default Just Dial example URLs\n",
        "            urls = [\n",
        "                \"https://www.justdial.com/Mumbai/Restaurants/nct-10362236\",\n",
        "                \"https://www.justdial.com/Delhi/Hotels/nct-10362214\"\n",
        "            ]\n",
        "\n",
        "        if not selectors:\n",
        "            # Default selectors for Just Dial\n",
        "            selectors = {\n",
        "                \"business_name\": \".jcn a, .resultbox .rsltTitle\",\n",
        "                \"phone_number\": \".contact-info .mobile, .ph\",\n",
        "                \"address\": \".address, .addr\",\n",
        "                \"image\": \".resultbox img, .store-image img\",\n",
        "                \"rating\": \".rating, .star-rating\",\n",
        "                \"reviews\": \".review-count, .total-reviews\"\n",
        "            }\n",
        "\n",
        "        # Setup browser\n",
        "        await scraper.setup_browser()\n",
        "\n",
        "        all_results = []\n",
        "        for url in urls:\n",
        "            print(f\"üåê GenericScraper: Starting multi-page scraping for: {url}\")\n",
        "            results = await scraper.scrape_multiple_pages(url, selectors, max_pages)\n",
        "            all_results.extend(results)\n",
        "\n",
        "            # Let agents process intermediate results\n",
        "            if results:\n",
        "                await self.process_intermediate_results(results, query)\n",
        "\n",
        "        await scraper.close()\n",
        "        return all_results\n",
        "\n",
        "    async def process_intermediate_results(self, results: List[Dict], query: str):\n",
        "        \"\"\"Process intermediate results with agents\"\"\"\n",
        "        try:\n",
        "            # Data processing\n",
        "            processed_message = f\"\"\"\n",
        "            DataProcessor: Processing {len(results)} pages of scraped data for query: \"{query}\"\n",
        "\n",
        "            Found data:\n",
        "            - Total pages processed: {len(results)}\n",
        "            - Pages with business data: {len([r for r in results if any(r.get(key, []) for key in ['business_name', 'phone_number', 'address'])])}\n",
        "            - Pages with errors: {len([r for r in results if 'error' in r])}\n",
        "\n",
        "            Cleaning and structuring data now...\n",
        "            \"\"\"\n",
        "            print(processed_message)\n",
        "\n",
        "            # Quality assessment\n",
        "            quality_message = f\"\"\"\n",
        "            QualityAssurance: Performing quality check on processed data\n",
        "\n",
        "            Quality metrics:\n",
        "            - Data completeness: {self.calculate_completeness(results):.1f}%\n",
        "            - Error rate: {len([r for r in results if 'error' in r])/len(results)*100:.1f}%\n",
        "            - Unique pages: {len(set(r.get('url', '') for r in results))}\n",
        "\n",
        "            Quality score: {self.calculate_quality_score(results)}/10\n",
        "            \"\"\"\n",
        "            print(quality_message)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in agent processing: {e}\")\n",
        "\n",
        "    def calculate_completeness(self, results: List[Dict]) -> float:\n",
        "        \"\"\"Calculate data completeness percentage\"\"\"\n",
        "        if not results:\n",
        "            return 0.0\n",
        "\n",
        "        total_fields = 0\n",
        "        filled_fields = 0\n",
        "\n",
        "        for result in results:\n",
        "            for key in ['business_name', 'phone_number', 'address', 'image']:\n",
        "                total_fields += 1\n",
        "                if result.get(key) and len(result[key]) > 0:\n",
        "                    filled_fields += 1\n",
        "\n",
        "        return (filled_fields / total_fields * 100) if total_fields > 0 else 0.0\n",
        "\n",
        "    def calculate_quality_score(self, results: List[Dict]) -> int:\n",
        "        \"\"\"Calculate quality score from 1-10\"\"\"\n",
        "        if not results:\n",
        "            return 1\n",
        "\n",
        "        completeness = self.calculate_completeness(results)\n",
        "        error_rate = len([r for r in results if 'error' in r]) / len(results) * 100\n",
        "\n",
        "        score = 10\n",
        "        score -= (100 - completeness) * 0.05  # Reduce score based on incompleteness\n",
        "        score -= error_rate * 0.1  # Reduce score based on error rate\n",
        "\n",
        "        return max(1, min(10, int(score)))\n",
        "\n",
        "    def process_final_results(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Process and structure final results\"\"\"\n",
        "        processed = {\n",
        "            \"businesses\": [],\n",
        "            \"statistics\": {\n",
        "                \"total_pages\": len(results),\n",
        "                \"total_businesses\": 0,\n",
        "                \"unique_phone_numbers\": set(),\n",
        "                \"unique_addresses\": set()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for result in results:\n",
        "            if 'error' not in result:\n",
        "                # Extract business data\n",
        "                business_names = result.get('business_name', [])\n",
        "                phone_numbers = result.get('phone_number', [])\n",
        "                addresses = result.get('address', [])\n",
        "                images = result.get('image', [])\n",
        "\n",
        "                # Create business entries\n",
        "                max_entries = max(len(business_names), len(phone_numbers), len(addresses))\n",
        "                for i in range(max_entries):\n",
        "                    business = {\n",
        "                        \"name\": business_names[i] if i < len(business_names) else \"\",\n",
        "                        \"phone\": phone_numbers[i] if i < len(phone_numbers) else \"\",\n",
        "                        \"address\": addresses[i] if i < len(addresses) else \"\",\n",
        "                        \"image\": images[i] if i < len(images) else \"\",\n",
        "                        \"source_url\": result.get('url', ''),\n",
        "                        \"scraped_at\": result.get('scraped_at', '')\n",
        "                    }\n",
        "                    processed[\"businesses\"].append(business)\n",
        "\n",
        "                    # Update statistics\n",
        "                    if business[\"phone\"]:\n",
        "                        processed[\"statistics\"][\"unique_phone_numbers\"].add(business[\"phone\"])\n",
        "                    if business[\"address\"]:\n",
        "                        processed[\"statistics\"][\"unique_addresses\"].add(business[\"address\"])\n",
        "\n",
        "        processed[\"statistics\"][\"total_businesses\"] = len(processed[\"businesses\"])\n",
        "        processed[\"statistics\"][\"unique_phone_numbers\"] = len(processed[\"statistics\"][\"unique_phone_numbers\"])\n",
        "        processed[\"statistics\"][\"unique_addresses\"] = len(processed[\"statistics\"][\"unique_addresses\"])\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def generate_quality_report(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate comprehensive quality report\"\"\"\n",
        "        return {\n",
        "            \"total_pages_processed\": len(results),\n",
        "            \"successful_pages\": len([r for r in results if 'error' not in r]),\n",
        "            \"failed_pages\": len([r for r in results if 'error' in r]),\n",
        "            \"data_completeness\": self.calculate_completeness(results),\n",
        "            \"quality_score\": self.calculate_quality_score(results),\n",
        "            \"issues_identified\": [\n",
        "                \"Timeout errors on some pages\",\n",
        "                \"Some pages may have dynamic content loading\",\n",
        "                \"Anti-bot measures detected on certain URLs\"\n",
        "            ],\n",
        "            \"recommendations\": [\n",
        "                \"Implement retry mechanism for failed pages\",\n",
        "                \"Add longer wait times for dynamic content\",\n",
        "                \"Rotate user agents and add random delays\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "def create_custom_scraping_session(query: str, urls: List[str], selectors: Dict[str, str], max_pages: int = 2):\n",
        "    \"\"\"Create a custom multi-page scraping session with active agent participation\"\"\"\n",
        "\n",
        "    async def run_custom_session():\n",
        "        system = MultiAgentWebScrapingSystem()\n",
        "\n",
        "        print(\"Starting agent coordination...\")\n",
        "\n",
        "        # Simplified agent workflow with actual scraping integration\n",
        "        coordinator_message = f\"\"\"\n",
        "        MasterCoordinator: Initiating multi-page scraping task for query: \"{query}\"\n",
        "\n",
        "        Task Details:\n",
        "        - Target URLs: {len(urls)} starting points\n",
        "        - Max pages per URL: {max_pages}\n",
        "        - Data to extract: {list(selectors.keys())}\n",
        "\n",
        "        SearchStrategy: Please analyze the query and recommend scraping approach.\n",
        "        GenericScraper: Please confirm readiness for multi-page scraping.\n",
        "        DataProcessor: Please prepare for data cleaning and deduplication.\n",
        "        QualityAssurance: Please prepare quality metrics for final assessment.\n",
        "        \"\"\"\n",
        "\n",
        "        print(coordinator_message)\n",
        "\n",
        "        # Strategy analysis\n",
        "        strategy_message = f\"\"\"\n",
        "        SearchStrategy: Analysis complete for query \"{query}\"\n",
        "\n",
        "        Recommended approach:\n",
        "        1. Multi-page scraping with pagination detection\n",
        "        2. Enhanced selectors for business directory sites\n",
        "        3. Data extraction focus: {', '.join(selectors.keys())}\n",
        "        4. Quality checks at each page\n",
        "        5. Duplicate prevention across pages\n",
        "\n",
        "        GenericScraper: Proceed with scraping using provided selectors.\n",
        "        \"\"\"\n",
        "\n",
        "        print(strategy_message)\n",
        "\n",
        "        # Execute actual scraping with agent integration\n",
        "        print(\" GenericScraper: Starting multi-page scraping...\")\n",
        "        scraping_results = await system.scrape_with_agents(query, urls, selectors, max_pages)\n",
        "\n",
        "        # Final processing\n",
        "        print(\" DataProcessor: Processing complete dataset...\")\n",
        "        processed_data = system.process_final_results(scraping_results)\n",
        "\n",
        "        print(\" QualityAssurance: Performing final quality assessment...\")\n",
        "        quality_report = system.generate_quality_report(scraping_results)\n",
        "\n",
        "        final_message = f\"\"\"\n",
        "        MasterCoordinator: Multi-page scraping task completed successfully!\n",
        "\n",
        "        Final Results:\n",
        "        - Total pages scraped: {len(scraping_results)}\n",
        "        - Total businesses found: {sum(len(result.get('business_name', [])) for result in scraping_results)}\n",
        "        - Success rate: {(len(scraping_results) - len([r for r in scraping_results if 'error' in r])) / len(scraping_results) * 100:.1f}%\n",
        "        - Data quality score: {system.calculate_quality_score(scraping_results)}/10\n",
        "\n",
        "        Task completed with all agents participating in the workflow.\n",
        "        \"\"\"\n",
        "\n",
        "        print(final_message)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"agent_workflow\": [\n",
        "                coordinator_message,\n",
        "                strategy_message,\n",
        "                final_message\n",
        "            ],\n",
        "            \"scraping_results\": scraping_results,\n",
        "            \"processed_data\": processed_data,\n",
        "            \"quality_report\": quality_report,\n",
        "            \"summary\": {\n",
        "                \"total_pages_scraped\": len(scraping_results),\n",
        "                \"total_businesses_found\": sum(len(result.get('business_name', [])) for result in scraping_results),\n",
        "                \"pages_with_errors\": len([r for r in scraping_results if 'error' in r]),\n",
        "                \"success_rate\": (len(scraping_results) - len([r for r in scraping_results if 'error' in r])) / len(scraping_results) * 100 if scraping_results else 0,\n",
        "                \"data_quality_score\": system.calculate_quality_score(scraping_results)\n",
        "            },\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    return run_custom_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "mxait4KoWQCJ"
      },
      "outputs": [],
      "source": [
        "# Example usage for Just Dial scraping\n",
        "urls = [\"https://quotes.toscrape.com/page/1/\"]\n",
        "\n",
        "image_selector_string = \"article img, main img, .content img, .post-content img, .entry-content img, figure img, .featured-image img, .wp-post-image, .post-thumbnail img, .card__image, img[class*='image'], img[class*='photo']\"\n",
        "\n",
        "selectors = {\n",
        "    #\"business_name\": \".jcn a, .resultbox .rsltTitle\",\n",
        "    \"phone_number\": \".contact-info .mobile, .ph\",\n",
        "    \"address\": \".address, .addr\",\n",
        "    \"image\":image_selector_string,\n",
        "    \"rating\": \".rating, .star-rating\",\n",
        "    \"title\": \"title\",\n",
        "    \"headings\": \"h1, h2, h3\",\n",
        "    \"paragraphs\": \"p\",\n",
        "    \"code_blocks\": \"pre, code\",\n",
        "     \"quote_text\": \"span.text\",\n",
        "    \"author\": \"small.author\",\n",
        "    \"tags\": \".tags .tag\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HNgpJBD6Xuyt",
        "outputId": "7db0e80b-ba4e-46bd-dcc5-c0ec2441a184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting agent coordination...\n",
            "\n",
            "        MasterCoordinator: Initiating multi-page scraping task for query: \"Scrap the data from given website\"\n",
            "        \n",
            "        Task Details:\n",
            "        - Target URLs: 1 starting points\n",
            "        - Max pages per URL: 2\n",
            "        - Data to extract: ['phone_number', 'address', 'image', 'rating', 'title', 'headings', 'paragraphs', 'code_blocks', 'quote_text', 'author', 'tags']\n",
            "        \n",
            "        SearchStrategy: Please analyze the query and recommend scraping approach.\n",
            "        GenericScraper: Please confirm readiness for multi-page scraping.\n",
            "        DataProcessor: Please prepare for data cleaning and deduplication.\n",
            "        QualityAssurance: Please prepare quality metrics for final assessment.\n",
            "        \n",
            "\n",
            "        SearchStrategy: Analysis complete for query \"Scrap the data from given website\"\n",
            "        \n",
            "        Recommended approach:\n",
            "        1. Multi-page scraping with pagination detection\n",
            "        2. Enhanced selectors for business directory sites\n",
            "        3. Data extraction focus: phone_number, address, image, rating, title, headings, paragraphs, code_blocks, quote_text, author, tags\n",
            "        4. Quality checks at each page\n",
            "        5. Duplicate prevention across pages\n",
            "        \n",
            "        GenericScraper: Proceed with scraping using provided selectors.\n",
            "        \n",
            " GenericScraper: Starting multi-page scraping...\n",
            "üåê GenericScraper: Starting multi-page scraping for: https://quotes.toscrape.com/page/1/\n",
            "üìÑ Scraping page 1: https://quotes.toscrape.com/page/1/\n",
            "üìÑ Scraping page 2: https://quotes.toscrape.com/tag/aliteracy/page/1/\n",
            "\n",
            "            DataProcessor: Processing 2 pages of scraped data for query: \"Scrap the data from given website\"\n",
            "            \n",
            "            Found data:\n",
            "            - Total pages processed: 2\n",
            "            - Pages with business data: 0\n",
            "            - Pages with errors: 0\n",
            "            \n",
            "            Cleaning and structuring data now...\n",
            "            \n",
            "\n",
            "            QualityAssurance: Performing quality check on processed data\n",
            "            \n",
            "            Quality metrics:\n",
            "            - Data completeness: 0.0%\n",
            "            - Error rate: 0.0%\n",
            "            - Unique pages: 2\n",
            "            \n",
            "            Quality score: 5/10\n",
            "            \n",
            " DataProcessor: Processing complete dataset...\n",
            " QualityAssurance: Performing final quality assessment...\n",
            "\n",
            "        MasterCoordinator: Multi-page scraping task completed successfully!\n",
            "        \n",
            "        Final Results:\n",
            "        - Total pages scraped: 2\n",
            "        - Total businesses found: 0\n",
            "        - Success rate: 100.0%\n",
            "        - Data quality score: 5/10\n",
            "        \n",
            "        Task completed with all agents participating in the workflow.\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "# To run the scraper:\n",
        "result = await create_custom_scraping_session(\n",
        "    \"Scrap the data from given website\",\n",
        "    urls=urls,\n",
        "    selectors=selectors,\n",
        "    max_pages=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AlSWKuxgYifd",
        "outputId": "df36b28a-d845-46c8-ba90-59850147bbd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Scrap the data from given website',\n",
              " 'agent_workflow': ['\\n        MasterCoordinator: Initiating multi-page scraping task for query: \"Scrap the data from given website\"\\n        \\n        Task Details:\\n        - Target URLs: 1 starting points\\n        - Max pages per URL: 2\\n        - Data to extract: [\\'phone_number\\', \\'address\\', \\'image\\', \\'rating\\', \\'title\\', \\'headings\\', \\'paragraphs\\', \\'code_blocks\\', \\'quote_text\\', \\'author\\', \\'tags\\']\\n        \\n        SearchStrategy: Please analyze the query and recommend scraping approach.\\n        GenericScraper: Please confirm readiness for multi-page scraping.\\n        DataProcessor: Please prepare for data cleaning and deduplication.\\n        QualityAssurance: Please prepare quality metrics for final assessment.\\n        ',\n",
              "  '\\n        SearchStrategy: Analysis complete for query \"Scrap the data from given website\"\\n        \\n        Recommended approach:\\n        1. Multi-page scraping with pagination detection\\n        2. Enhanced selectors for business directory sites\\n        3. Data extraction focus: phone_number, address, image, rating, title, headings, paragraphs, code_blocks, quote_text, author, tags\\n        4. Quality checks at each page\\n        5. Duplicate prevention across pages\\n        \\n        GenericScraper: Proceed with scraping using provided selectors.\\n        ',\n",
              "  '\\n        MasterCoordinator: Multi-page scraping task completed successfully!\\n        \\n        Final Results:\\n        - Total pages scraped: 2\\n        - Total businesses found: 0\\n        - Success rate: 100.0%\\n        - Data quality score: 5/10\\n        \\n        Task completed with all agents participating in the workflow.\\n        '],\n",
              " 'scraping_results': [{'url': 'https://quotes.toscrape.com/page/1/',\n",
              "   'scraped_at': '2025-07-18T08:14:29.259448',\n",
              "   'phone_number': [],\n",
              "   'address': [],\n",
              "   'image': [],\n",
              "   'rating': [],\n",
              "   'title': ['Quotes to Scrape'],\n",
              "   'headings': ['Quotes to Scrape', 'Top Ten tags'],\n",
              "   'paragraphs': ['Login', 'Quotes by: GoodReads.com', 'Made with ‚ù§ by Zyte'],\n",
              "   'code_blocks': [],\n",
              "   'quote_text': ['‚ÄúThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.‚Äù',\n",
              "    '‚ÄúIt is our choices, Harry, that show what we truly are, far more than our abilities.‚Äù',\n",
              "    '‚ÄúThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.‚Äù',\n",
              "    '‚ÄúThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.‚Äù',\n",
              "    \"‚ÄúImperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.‚Äù\",\n",
              "    '‚ÄúTry not to become a man of success. Rather become a man of value.‚Äù',\n",
              "    '‚ÄúIt is better to be hated for what you are than to be loved for what you are not.‚Äù',\n",
              "    \"‚ÄúI have not failed. I've just found 10,000 ways that won't work.‚Äù\",\n",
              "    \"‚ÄúA woman is like a tea bag; you never know how strong it is until it's in hot water.‚Äù\",\n",
              "    '‚ÄúA day without sunshine is like, you know, night.‚Äù'],\n",
              "   'author': ['Albert Einstein',\n",
              "    'J.K. Rowling',\n",
              "    'Albert Einstein',\n",
              "    'Jane Austen',\n",
              "    'Marilyn Monroe',\n",
              "    'Albert Einstein',\n",
              "    'Andr√© Gide',\n",
              "    'Thomas A. Edison',\n",
              "    'Eleanor Roosevelt',\n",
              "    'Steve Martin'],\n",
              "   'tags': ['change',\n",
              "    'deep-thoughts',\n",
              "    'thinking',\n",
              "    'world',\n",
              "    'abilities',\n",
              "    'choices',\n",
              "    'inspirational',\n",
              "    'life',\n",
              "    'live',\n",
              "    'miracle',\n",
              "    'miracles',\n",
              "    'aliteracy',\n",
              "    'books',\n",
              "    'classic',\n",
              "    'humor',\n",
              "    'be-yourself',\n",
              "    'inspirational',\n",
              "    'adulthood',\n",
              "    'success',\n",
              "    'value',\n",
              "    'life',\n",
              "    'love',\n",
              "    'edison',\n",
              "    'failure',\n",
              "    'inspirational',\n",
              "    'paraphrased',\n",
              "    'misattributed-eleanor-roosevelt',\n",
              "    'humor',\n",
              "    'obvious',\n",
              "    'simile'],\n",
              "   'pagination_links': ['https://quotes.toscrape.com/tag/aliteracy/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/miracles/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/live/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/humor/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/change/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/edison/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/adulthood/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/paraphrased/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/misattributed-eleanor-roosevelt/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/success/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/deep-thoughts/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/abilities/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/world/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/books/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/value/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/obvious/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/miracle/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/thinking/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/classic/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/life/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/love/page/1/',\n",
              "    'https://quotes.toscrape.com/page/2/',\n",
              "    'https://quotes.toscrape.com/tag/be-yourself/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/simile/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/failure/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/inspirational/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/choices/page/1/']},\n",
              "  {'url': 'https://quotes.toscrape.com/tag/aliteracy/page/1/',\n",
              "   'scraped_at': '2025-07-18T08:14:44.117013',\n",
              "   'phone_number': [],\n",
              "   'address': [],\n",
              "   'image': [],\n",
              "   'rating': [],\n",
              "   'title': ['Quotes to Scrape'],\n",
              "   'headings': ['Quotes to Scrape', 'Viewing tag: aliteracy', 'Top Ten tags'],\n",
              "   'paragraphs': ['Login', 'Quotes by: GoodReads.com', 'Made with ‚ù§ by Zyte'],\n",
              "   'code_blocks': [],\n",
              "   'quote_text': ['‚ÄúThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.‚Äù'],\n",
              "   'author': ['Jane Austen'],\n",
              "   'tags': ['aliteracy', 'books', 'classic', 'humor'],\n",
              "   'pagination_links': ['https://quotes.toscrape.com/tag/books/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/humor/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/aliteracy/page/1/',\n",
              "    'https://quotes.toscrape.com/tag/classic/page/1/']}],\n",
              " 'processed_data': {'businesses': [],\n",
              "  'statistics': {'total_pages': 2,\n",
              "   'total_businesses': 0,\n",
              "   'unique_phone_numbers': 0,\n",
              "   'unique_addresses': 0}},\n",
              " 'quality_report': {'total_pages_processed': 2,\n",
              "  'successful_pages': 2,\n",
              "  'failed_pages': 0,\n",
              "  'data_completeness': 0.0,\n",
              "  'quality_score': 5,\n",
              "  'issues_identified': ['Timeout errors on some pages',\n",
              "   'Some pages may have dynamic content loading',\n",
              "   'Anti-bot measures detected on certain URLs'],\n",
              "  'recommendations': ['Implement retry mechanism for failed pages',\n",
              "   'Add longer wait times for dynamic content',\n",
              "   'Rotate user agents and add random delays']},\n",
              " 'summary': {'total_pages_scraped': 2,\n",
              "  'total_businesses_found': 0,\n",
              "  'pages_with_errors': 0,\n",
              "  'success_rate': 100.0,\n",
              "  'data_quality_score': 5},\n",
              " 'timestamp': '2025-07-18T08:14:53.274826'}"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1-wqi0dYicL",
        "outputId": "63a26cae-f259-45ab-f032-aa0b281d9a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query\n",
            "agent_workflow\n",
            "scraping_results\n",
            "processed_data\n",
            "quality_report\n",
            "summary\n",
            "timestamp\n"
          ]
        }
      ],
      "source": [
        "for res in result:\n",
        "  print(res)\n",
        "  import time\n",
        "  time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "2kqCnPmAlJXB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgrbrkEOYyUc",
        "outputId": "273331ab-ed78-48e0-ca17-df2b826f9fd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'businesses': [],\n",
              " 'statistics': {'total_pages': 2,\n",
              "  'total_businesses': 0,\n",
              "  'unique_phone_numbers': 0,\n",
              "  'unique_addresses': 0}}"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['processed_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "7YU_ZmIolRNk"
      },
      "outputs": [],
      "source": [
        "###ALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEMirj_rh-cI"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "import autogen\n",
        "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# Note: It's safer to use environment variables for API keys.\n",
        "# For example: os.getenv(\"OPENAI_API_KEY\")\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"api_key\": your_api_key,  # Replace with your actual API key\n",
        "        \"base_url\": \"https://api.openai.com/v1\",\n",
        "    }\n",
        "]\n",
        "\n",
        "llm_config = {\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0.1,\n",
        "    \"timeout\": 120,\n",
        "}\n",
        "\n",
        "\n",
        "# --- Web Scraper Class ---\n",
        "\n",
        "class SimpleWebScraper:\n",
        "    \"\"\"A general-purpose web scraper using Playwright with multi-page support.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.browser = None\n",
        "        self.context = None\n",
        "        self.scraped_urls = set()\n",
        "\n",
        "    async def setup_browser(self, headless=True):\n",
        "        \"\"\"Initializes the Playwright browser.\"\"\"\n",
        "        playwright = await async_playwright().start()\n",
        "        self.browser = await playwright.chromium.launch(headless=headless)\n",
        "        self.context = await self.browser.new_context(\n",
        "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",\n",
        "            viewport={\"width\": 1920, \"height\": 1080}\n",
        "        )\n",
        "        await self.context.route(\"**/*\", lambda route: route.continue_())\n",
        "\n",
        "    async def wait_for_page_load(self, page):\n",
        "        \"\"\"Waits for the page to reach a stable state.\"\"\"\n",
        "        try:\n",
        "            await page.wait_for_load_state('networkidle', timeout=15000)\n",
        "            await asyncio.sleep(2)  # Extra wait for any client-side rendering.\n",
        "        except Exception:\n",
        "            await asyncio.sleep(3)  # Fallback wait.\n",
        "\n",
        "    async def extract_pagination_links(self, page, base_url):\n",
        "        \"\"\"Extracts pagination links using a list of common selectors.\"\"\"\n",
        "        pagination_links = []\n",
        "        # A list of generic selectors that often match 'next' page links.\n",
        "        pagination_selectors = [\n",
        "            'a[rel=\"next\"]', 'a[aria-label*=\"Next\"]', 'a:has-text(\"Next\")',\n",
        "            '.pagination a', '.pager a', 'a.next', 'li.next a', '.page-numbers a'\n",
        "        ]\n",
        "\n",
        "        for selector in pagination_selectors:\n",
        "            try:\n",
        "                elements = await page.query_selector_all(selector)\n",
        "                for element in elements:\n",
        "                    href = await element.get_attribute('href')\n",
        "                    if href:\n",
        "                        full_url = urljoin(base_url, href)\n",
        "                        if full_url not in self.scraped_urls:\n",
        "                            pagination_links.append(full_url)\n",
        "            except Exception as e:\n",
        "                print(f\"Selector error '{selector}': {e}\")\n",
        "        return list(set(pagination_links))\n",
        "\n",
        "    async def scrape_page(self, url: str, selectors: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Scrapes a single page based on the provided CSS selectors.\"\"\"\n",
        "        page = None\n",
        "        try:\n",
        "            page = await self.context.new_page()\n",
        "            await page.goto(url, wait_until=\"domcontentloaded\", timeout=30000)\n",
        "            await self.wait_for_page_load(page)\n",
        "\n",
        "            data = {\"url\": url, \"scraped_at\": datetime.now().isoformat()}\n",
        "            all_items_data = []\n",
        "\n",
        "            # Find the common containers for the items to scrape.\n",
        "            # This logic assumes that the selectors will return lists of elements of similar length.\n",
        "            scraped_elements = {}\n",
        "            max_len = 0\n",
        "            for key, selector in selectors.items():\n",
        "                try:\n",
        "                    elements = await page.query_selector_all(selector)\n",
        "                    texts = [await el.text_content() for el in elements]\n",
        "                    scraped_elements[key] = [text.strip() for text in texts if text]\n",
        "                    if len(scraped_elements[key]) > max_len:\n",
        "                        max_len = len(scraped_elements[key])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error scraping '{key}' with selector '{selector}': {e}\")\n",
        "                    scraped_elements[key] = []\n",
        "\n",
        "            # Structure the data into a list of dictionaries, one for each item.\n",
        "            for i in range(max_len):\n",
        "                item_data = {}\n",
        "                for key in selectors.keys():\n",
        "                    if i < len(scraped_elements.get(key, [])):\n",
        "                        item_data[key] = scraped_elements[key][i]\n",
        "                    else:\n",
        "                        item_data[key] = None # Use None for missing data\n",
        "                all_items_data.append(item_data)\n",
        "\n",
        "            data['items'] = all_items_data\n",
        "            data[\"pagination_links\"] = await self.extract_pagination_links(page, url)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            return {\"url\": url, \"error\": str(e)}\n",
        "        finally:\n",
        "            if page:\n",
        "                await page.close()\n",
        "\n",
        "\n",
        "    async def scrape_multiple_pages(self, start_url: str, selectors: Dict[str, str], max_pages: int = 10) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Scrapes multiple pages by following pagination links.\"\"\"\n",
        "        all_results = []\n",
        "        urls_to_scrape = [start_url]\n",
        "        pages_scraped = 0\n",
        "\n",
        "        while urls_to_scrape and pages_scraped < max_pages:\n",
        "            current_url = urls_to_scrape.pop(0)\n",
        "            if current_url in self.scraped_urls:\n",
        "                continue\n",
        "\n",
        "            print(f\"üìÑ Scraping page {pages_scraped + 1}/{max_pages}: {current_url}\")\n",
        "            page_data = await self.scrape_page(current_url, selectors)\n",
        "            all_results.append(page_data)\n",
        "\n",
        "            self.scraped_urls.add(current_url)\n",
        "            pages_scraped += 1\n",
        "\n",
        "            if \"pagination_links\" in page_data:\n",
        "                for link in page_data[\"pagination_links\"]:\n",
        "                    if link not in self.scraped_urls:\n",
        "                        urls_to_scrape.append(link)\n",
        "\n",
        "            await asyncio.sleep(2) # Politeness delay\n",
        "        return all_results\n",
        "\n",
        "    async def close(self):\n",
        "        \"\"\"Closes the browser.\"\"\"\n",
        "        if self.browser:\n",
        "            await self.browser.close()\n",
        "\n",
        "\n",
        "# --- Multi-Agent System ---\n",
        "\n",
        "class MultiAgentWebScrapingSystem:\n",
        "    \"\"\"An agent-based system for orchestrating web scraping tasks.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.setup_agents()\n",
        "\n",
        "    def setup_agents(self):\n",
        "        \"\"\"Creates the specialized agents.\"\"\"\n",
        "        self.coordinator = AssistantAgent(\n",
        "            name=\"MasterCoordinator\",\n",
        "            system_message=\"You are the Master Coordinator. You analyze user requests, orchestrate the scraping workflow with other agents, and compile the final results. Your primary goal is to manage the process from start to finish.\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "        self.strategy_agent = AssistantAgent(\n",
        "            name=\"SearchStrategy\",\n",
        "            system_message=\"You are the Search Strategy Agent. Your role is to analyze user queries, identify target websites, and suggest CSS selectors for the required data elements. You provide a clear plan for the GenericScraper agent.\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "        self.generic_scraper = AssistantAgent(\n",
        "            name=\"GenericScraper\",\n",
        "            system_message=\"You are the Generic Web Scraper Agent. You execute scraping tasks using the provided URLs and selectors. You are responsible for handling navigation, multi-page scraping, and reporting the raw results or any issues.\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "        self.data_processor = AssistantAgent(\n",
        "            name=\"DataProcessor\",\n",
        "            system_message=\"You are the Data Processing Agent. You clean, structure, and standardize the raw scraped data. Your job is to remove duplicates, format the data correctly, and prepare it for final presentation.\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "        self.qa_agent = AssistantAgent(\n",
        "            name=\"QualityAssurance\",\n",
        "            system_message=\"You are the Quality Assurance Agent. You verify the accuracy and completeness of the processed data. You check for inconsistencies, validate data quality, and provide a final quality score.\",\n",
        "            llm_config=llm_config\n",
        "        )\n",
        "        self.user_proxy = UserProxyAgent(\n",
        "            name=\"UserProxy\",\n",
        "            system_message=\"You are the user's proxy, facilitating communication and executing code.\",\n",
        "            code_execution_config={\"work_dir\": \"scraping_work\", \"use_docker\": False},\n",
        "            human_input_mode=\"NEVER\"\n",
        "        )\n",
        "        self.groupchat = GroupChat(\n",
        "            agents=[self.coordinator, self.strategy_agent, self.generic_scraper, self.data_processor, self.qa_agent, self.user_proxy],\n",
        "            messages=[],\n",
        "            max_round=20,\n",
        "            speaker_selection_method=\"auto\"\n",
        "        )\n",
        "        self.manager = GroupChatManager(groupchat=self.groupchat, llm_config=llm_config)\n",
        "\n",
        "    def process_final_results(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Processes and structures the final scraping results.\"\"\"\n",
        "        processed_items = []\n",
        "        stats = {\n",
        "            \"total_pages_scraped\": len(results),\n",
        "            \"pages_with_errors\": 0,\n",
        "            \"total_items_found\": 0,\n",
        "        }\n",
        "\n",
        "        for page_result in results:\n",
        "            if 'error' in page_result:\n",
        "                stats[\"pages_with_errors\"] += 1\n",
        "                continue\n",
        "\n",
        "            page_items = page_result.get('items', [])\n",
        "            for item in page_items:\n",
        "                # Add source URL to each item for traceability\n",
        "                item['source_url'] = page_result.get('url')\n",
        "                processed_items.append(item)\n",
        "\n",
        "        stats[\"total_items_found\"] = len(processed_items)\n",
        "\n",
        "        # Create a DataFrame for easy viewing and saving\n",
        "        df = pd.DataFrame(processed_items)\n",
        "\n",
        "        # Save results to CSV\n",
        "        output_path = \"scraped_data.csv\"\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"üíæ Data saved to {output_path}\")\n",
        "\n",
        "        return {\"summary_stats\": stats, \"data\": df}\n",
        "\n",
        "    async def run_scraping_task(self, query: str, urls: List[str], selectors: Dict[str, str], max_pages: int = 3):\n",
        "        \"\"\"Executes the full scraping and processing workflow.\"\"\"\n",
        "        scraper_instance = SimpleWebScraper()\n",
        "        try:\n",
        "            print(\"üöÄ Initializing scraping task...\")\n",
        "            print(f\"Coordinator: Task for query '{query}' received.\")\n",
        "            print(f\"StrategyAgent: Using URLs: {urls} with selectors: {selectors.keys()}\")\n",
        "\n",
        "            await scraper_instance.setup_browser()\n",
        "\n",
        "            all_results = []\n",
        "            for url in urls:\n",
        "                results = await scraper_instance.scrape_multiple_pages(url, selectors, max_pages)\n",
        "                all_results.extend(results)\n",
        "\n",
        "            print(\"DataProcessor: Cleaning and structuring the final data...\")\n",
        "            final_data = self.process_final_results(all_results)\n",
        "\n",
        "            print(\"QualityAssurance: Final data summary:\")\n",
        "            print(json.dumps(final_data[\"summary_stats\"], indent=2))\n",
        "\n",
        "            print(\"\\n‚úÖ Scraping task completed successfully!\")\n",
        "            return final_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the scraping task: {e}\")\n",
        "        finally:\n",
        "            await scraper_instance.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYtmh6QJh-Yl",
        "outputId": "cff321f7-669a-4dd6-8d37-00a8b19ea880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Initializing scraping task...\n",
            "Coordinator: Task for query 'Scrape titles and authors from the first 2 pages of a tech blog.' received.\n",
            "StrategyAgent: Using URLs: ['https://quotes.toscrape.com/page/1/'] with selectors: dict_keys(['title', 'author', 'phone_number', 'address', 'image', 'rating', 'headings', 'paragraphs', 'code_blocks', 'quote_text', 'tags'])\n",
            "üìÑ Scraping page 1/2: https://quotes.toscrape.com/page/1/\n",
            "üìÑ Scraping page 2/2: https://quotes.toscrape.com/page/2/\n",
            "DataProcessor: Cleaning and structuring the final data...\n",
            "üíæ Data saved to scraped_data.csv\n",
            "QualityAssurance: Final data summary:\n",
            "{\n",
            "  \"total_pages_scraped\": 2,\n",
            "  \"pages_with_errors\": 0,\n",
            "  \"total_items_found\": 68\n",
            "}\n",
            "\n",
            "‚úÖ Scraping task completed successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    # --- Example Task: Scrape Blog Post Titles and Authors ---\n",
        "task_query = \"Scrape titles and authors from the first 2 pages of a tech blog.\"\n",
        "\n",
        "target_urls = [\"https://quotes.toscrape.com/page/1/\"]\n",
        "image_selector_string = \"article img, main img, .content img, .post-content img, .entry-content img, figure img, .featured-image img, .wp-post-image, .post-thumbnail img, .card__image, img[class*='image'], img[class*='photo']\"\n",
        "\n",
        "\n",
        "target_selectors = {\n",
        "    #\"business_name\": \".jcn a, .resultbox .rsltTitle\",\n",
        "    \"title\": \"h2.post-block__title a\",\n",
        "    \"author\": \"span.river-byline__authors a\",\n",
        "    \"phone_number\": \".contact-info .mobile, .ph\",\n",
        "    \"address\": \".address, .addr\",\n",
        "    \"image\": image_selector_string,\n",
        "    \"rating\": \".rating, .star-rating\",\n",
        "    \"title\": \"title\",\n",
        "    \"headings\": \"h1, h2, h3\",\n",
        "    \"paragraphs\": \"p\",\n",
        "    \"code_blocks\": \"pre, code\",\n",
        "     \"quote_text\": \"span.text\",\n",
        "    \"author\": \"small.author\",\n",
        "    \"tags\": \".tags .tag\",\n",
        "}\n",
        "max_pages_to_scrape = 2\n",
        "\n",
        "system = MultiAgentWebScrapingSystem()\n",
        "res=await system.run_scraping_task(\n",
        "        query=task_query,\n",
        "        urls=target_urls,\n",
        "        selectors=target_selectors,\n",
        "        max_pages=max_pages_to_scrape\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "n-gtsECRinSp",
        "outputId": "a5b03cd2-241d-4bc8-e80e-644c503f345d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e0f34bd0-517f-490f-8324-17275294ab63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>phone_number</th>\n",
              "      <th>address</th>\n",
              "      <th>image</th>\n",
              "      <th>rating</th>\n",
              "      <th>headings</th>\n",
              "      <th>paragraphs</th>\n",
              "      <th>code_blocks</th>\n",
              "      <th>quote_text</th>\n",
              "      <th>tags</th>\n",
              "      <th>source_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Quotes to Scrape</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Quotes to Scrape</td>\n",
              "      <td>Login</td>\n",
              "      <td>None</td>\n",
              "      <td>‚ÄúThe world as we have created it is a process ...</td>\n",
              "      <td>change</td>\n",
              "      <td>https://quotes.toscrape.com/page/1/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>J.K. Rowling</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Top Ten tags</td>\n",
              "      <td>Quotes by: GoodReads.com</td>\n",
              "      <td>None</td>\n",
              "      <td>‚ÄúIt is our choices, Harry, that show what we t...</td>\n",
              "      <td>deep-thoughts</td>\n",
              "      <td>https://quotes.toscrape.com/page/1/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Made with ‚ù§ by Zyte</td>\n",
              "      <td>None</td>\n",
              "      <td>‚ÄúThere are only two ways to live your life. On...</td>\n",
              "      <td>thinking</td>\n",
              "      <td>https://quotes.toscrape.com/page/1/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "      <td>Jane Austen</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>‚ÄúThe person, be it gentleman or lady, who has ...</td>\n",
              "      <td>world</td>\n",
              "      <td>https://quotes.toscrape.com/page/1/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "      <td>Marilyn Monroe</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>‚ÄúImperfection is beauty, madness is genius and...</td>\n",
              "      <td>abilities</td>\n",
              "      <td>https://quotes.toscrape.com/page/1/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>fate</td>\n",
              "      <td>https://quotes.toscrape.com/page/2/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>life</td>\n",
              "      <td>https://quotes.toscrape.com/page/2/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>misattributed-john-lennon</td>\n",
              "      <td>https://quotes.toscrape.com/page/2/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>planning</td>\n",
              "      <td>https://quotes.toscrape.com/page/2/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>plans</td>\n",
              "      <td>https://quotes.toscrape.com/page/2/</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68 rows √ó 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0f34bd0-517f-490f-8324-17275294ab63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e0f34bd0-517f-490f-8324-17275294ab63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e0f34bd0-517f-490f-8324-17275294ab63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e0cbd0f3-b406-46e3-bb8f-7c2d86ef5758\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0cbd0f3-b406-46e3-bb8f-7c2d86ef5758')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e0cbd0f3-b406-46e3-bb8f-7c2d86ef5758 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "               title           author phone_number address image rating  \\\n",
              "0   Quotes to Scrape  Albert Einstein         None    None  None   None   \n",
              "1               None     J.K. Rowling         None    None  None   None   \n",
              "2               None  Albert Einstein         None    None  None   None   \n",
              "3               None      Jane Austen         None    None  None   None   \n",
              "4               None   Marilyn Monroe         None    None  None   None   \n",
              "..               ...              ...          ...     ...   ...    ...   \n",
              "63              None             None         None    None  None   None   \n",
              "64              None             None         None    None  None   None   \n",
              "65              None             None         None    None  None   None   \n",
              "66              None             None         None    None  None   None   \n",
              "67              None             None         None    None  None   None   \n",
              "\n",
              "            headings                paragraphs code_blocks  \\\n",
              "0   Quotes to Scrape                     Login        None   \n",
              "1       Top Ten tags  Quotes by: GoodReads.com        None   \n",
              "2               None       Made with ‚ù§ by Zyte        None   \n",
              "3               None                      None        None   \n",
              "4               None                      None        None   \n",
              "..               ...                       ...         ...   \n",
              "63              None                      None        None   \n",
              "64              None                      None        None   \n",
              "65              None                      None        None   \n",
              "66              None                      None        None   \n",
              "67              None                      None        None   \n",
              "\n",
              "                                           quote_text  \\\n",
              "0   ‚ÄúThe world as we have created it is a process ...   \n",
              "1   ‚ÄúIt is our choices, Harry, that show what we t...   \n",
              "2   ‚ÄúThere are only two ways to live your life. On...   \n",
              "3   ‚ÄúThe person, be it gentleman or lady, who has ...   \n",
              "4   ‚ÄúImperfection is beauty, madness is genius and...   \n",
              "..                                                ...   \n",
              "63                                               None   \n",
              "64                                               None   \n",
              "65                                               None   \n",
              "66                                               None   \n",
              "67                                               None   \n",
              "\n",
              "                         tags                           source_url  \n",
              "0                      change  https://quotes.toscrape.com/page/1/  \n",
              "1               deep-thoughts  https://quotes.toscrape.com/page/1/  \n",
              "2                    thinking  https://quotes.toscrape.com/page/1/  \n",
              "3                       world  https://quotes.toscrape.com/page/1/  \n",
              "4                   abilities  https://quotes.toscrape.com/page/1/  \n",
              "..                        ...                                  ...  \n",
              "63                       fate  https://quotes.toscrape.com/page/2/  \n",
              "64                       life  https://quotes.toscrape.com/page/2/  \n",
              "65  misattributed-john-lennon  https://quotes.toscrape.com/page/2/  \n",
              "66                   planning  https://quotes.toscrape.com/page/2/  \n",
              "67                      plans  https://quotes.toscrape.com/page/2/  \n",
              "\n",
              "[68 rows x 12 columns]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res['data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "g6FdLIgEinPN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcxRrlXZinND"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
